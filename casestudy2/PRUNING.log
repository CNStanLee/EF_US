Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.4508, Train Acc: 70.34%, Val Loss: 0.5462, Val Acc: 84.92%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4017, Train Acc: 88.30%, Val Loss: 0.3343, Val Acc: 90.42%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3133, Train Acc: 90.94%, Val Loss: 0.2952, Val Acc: 91.17%
Epoch [4/10], LR: 0.001000, Train Loss: 0.2985, Train Acc: 91.35%, Val Loss: 0.2897, Val Acc: 91.70%
Epoch [5/10], LR: 0.000100, Train Loss: 0.2944, Train Acc: 91.45%, Val Loss: 0.2865, Val Acc: 91.70%
Epoch [6/10], LR: 0.000100, Train Loss: 0.2896, Train Acc: 91.61%, Val Loss: 0.2966, Val Acc: 91.67%
Epoch [7/10], LR: 0.000010, Train Loss: 0.2878, Train Acc: 91.62%, Val Loss: 0.2891, Val Acc: 91.81%
Epoch [8/10], LR: 0.000010, Train Loss: 0.2859, Train Acc: 91.69%, Val Loss: 0.2980, Val Acc: 91.26%
Epoch [9/10], LR: 0.000001, Train Loss: 0.2860, Train Acc: 91.60%, Val Loss: 0.2767, Val Acc: 91.80%
Epoch [10/10], LR: 0.000001, Train Loss: 0.2846, Train Acc: 91.58%, Val Loss: 0.3025, Val Acc: 91.28%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   504 |      79.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 9,120 |      81.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,714 |      83.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   126 |      85.00%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 11,986 |      80.65%

Compression Ratio (pruning only): 5.17x
Test Accuracy after re-training: 92.43%
Accuracy drop after pruning and re-training: 6.35%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 42
Layer conv_features.1.weight - Safe pruning rate: 72.0%
Layer conv_features.5.weight - Safe pruning rate: 79.0%
Layer conv_features.9.weight - Safe pruning rate: 82.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 86.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.72 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.6575, Train Acc: 67.77%, Val Loss: 0.5822, Val Acc: 83.46%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4587, Train Acc: 86.84%, Val Loss: 0.3516, Val Acc: 89.78%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3581, Train Acc: 89.61%, Val Loss: 0.3446, Val Acc: 90.18%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3385, Train Acc: 90.27%, Val Loss: 0.3333, Val Acc: 90.52%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3323, Train Acc: 90.45%, Val Loss: 0.3329, Val Acc: 90.53%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3279, Train Acc: 90.53%, Val Loss: 0.3388, Val Acc: 90.18%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3257, Train Acc: 90.60%, Val Loss: 0.3293, Val Acc: 90.22%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3308, Train Acc: 90.57%, Val Loss: 0.3224, Val Acc: 90.96%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3273, Train Acc: 90.64%, Val Loss: 0.3311, Val Acc: 90.92%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3221, Train Acc: 90.57%, Val Loss: 0.3231, Val Acc: 90.69%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   504 |      79.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,640 |      82.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   118 |      85.95%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 11,296 |      81.76%

Compression Ratio (pruning only): 5.48x
Test Accuracy after re-training: 91.50%
Accuracy drop after pruning and re-training: 7.29%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 44
Layer conv_features.1.weight - Safe pruning rate: 74.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 83.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 87.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.74 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.8363, Train Acc: 64.59%, Val Loss: 0.6557, Val Acc: 80.69%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4918, Train Acc: 85.54%, Val Loss: 0.3946, Val Acc: 88.39%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3800, Train Acc: 88.90%, Val Loss: 0.3724, Val Acc: 89.20%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3642, Train Acc: 89.30%, Val Loss: 0.3514, Val Acc: 89.51%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3504, Train Acc: 89.92%, Val Loss: 0.3543, Val Acc: 89.75%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3503, Train Acc: 89.69%, Val Loss: 0.3520, Val Acc: 89.88%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3465, Train Acc: 89.81%, Val Loss: 0.3420, Val Acc: 89.85%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3563, Train Acc: 89.78%, Val Loss: 0.3585, Val Acc: 89.83%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3597, Train Acc: 89.46%, Val Loss: 0.3553, Val Acc: 89.81%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3492, Train Acc: 89.74%, Val Loss: 0.3468, Val Acc: 89.90%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,160 |      83.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   109 |      87.02%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,783 |      82.59%

Compression Ratio (pruning only): 5.74x
Test Accuracy after re-training: 90.46%
Accuracy drop after pruning and re-training: 8.33%
end.
Final Test Accuracy: 91.51%

Pruning Stage 5: Final Retraining
Epoch [1/200], LR: 0.010000, Train Loss: 0.2711, Train Acc: 92.08%, Val Loss: 0.2401, Val Acc: 92.59%
Epoch [2/200], LR: 0.010000, Train Loss: 0.1971, Train Acc: 93.99%, Val Loss: 0.1846, Val Acc: 94.53%
Epoch [3/200], LR: 0.010000, Train Loss: 0.1647, Train Acc: 94.90%, Val Loss: 0.1598, Val Acc: 95.38%
Epoch [4/200], LR: 0.010000, Train Loss: 0.1451, Train Acc: 95.55%, Val Loss: 0.1464, Val Acc: 95.63%
Epoch [5/200], LR: 0.010000, Train Loss: 0.1324, Train Acc: 95.87%, Val Loss: 0.1289, Val Acc: 95.92%
Epoch [6/200], LR: 0.010000, Train Loss: 0.1262, Train Acc: 96.02%, Val Loss: 0.1185, Val Acc: 96.41%
Epoch [7/200], LR: 0.010000, Train Loss: 0.1186, Train Acc: 96.31%, Val Loss: 0.1346, Val Acc: 95.92%
^CTraceback (most recent call last):
  File "/home/changhong/prj/finn/script/EF_US/casestudy2/pruning.py", line 793, in <module>
    auto_prune_model(ori_model, model_name, w, a, dataset_name='MNIST', pruning_type='l1')
  File "/home/changhong/prj/finn/script/EF_US/casestudy2/pruning.py", line 659, in auto_prune_model
    model = auto_prune_model_sensitivity(ori_model = ori_model, model_name = model_name, w = w, a = a, dataset_name=dataset_name, pruning_type = pruning_type)
  File "/home/changhong/prj/finn/script/EF_US/casestudy2/pruning.py", line 749, in auto_prune_model_sensitivity
    final_model2 = retrain_model(final_model, train_loader, val_loader, epochs=final_retrain_epochs)
  File "/home/changhong/prj/finn/script/EF_US/casestudy2/pruning.py", line 399, in retrain_model
    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)
  File "/home/changhong/prj/finn/script/EF_US/casestudy2/train.py", line 92, in train
    loss.backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

root@finn_dev_root:/home/changhong/prj/finn/script/EF_US/casestudy2# python pruning.py 
Data directory: ./data
Using device: cuda
Current working directory: /home/changhong/prj/finn/script/EF_US/casestudy2
Using device: cuda
Data directory: ./data
Build directory: ./build
Model directory: ./model
Using device: cuda
Using device: cuda
Analyzing model sparsity...
pruning with sens
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
------------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    150 |       0.00%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |      6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |      6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |      1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |  2,400 |       0.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |     16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |     16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |      1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 48,000 |       0.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |    120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |    120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |      1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 10,080 |       0.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |     84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |     84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |      1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    840 |       0.00%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |      1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |      1 |       0.00%
------------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 61,928 |       0.00%

Compression Ratio (pruning only): 1.00x
Dataset split complete:
Training set: 48000 samples
Validation set: 12000 samples
Test set: 10000 samples

Pruning Stage 1: Sensitivity Analysis
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)
  return super(Tensor, self).rename(names)
Original Test Accuracy: 98.79%

Pruning Stage 2: Pruning with Specific Rates
Layer conv_features.1.weight - Safe pruning rate: 72.0%
Layer conv_features.5.weight - Safe pruning rate: 79.0%
Layer conv_features.9.weight - Safe pruning rate: 82.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 86.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.72 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.6905, Train Acc: 67.15%, Val Loss: 0.6104, Val Acc: 82.69%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4531, Train Acc: 87.15%, Val Loss: 0.3563, Val Acc: 89.97%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3575, Train Acc: 89.66%, Val Loss: 0.3428, Val Acc: 90.45%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3389, Train Acc: 90.34%, Val Loss: 0.3300, Val Acc: 90.59%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3298, Train Acc: 90.65%, Val Loss: 0.3298, Val Acc: 90.38%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3267, Train Acc: 90.66%, Val Loss: 0.3282, Val Acc: 90.72%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3255, Train Acc: 90.53%, Val Loss: 0.3231, Val Acc: 90.67%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3190, Train Acc: 90.79%, Val Loss: 0.3158, Val Acc: 90.97%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3211, Train Acc: 90.85%, Val Loss: 0.3179, Val Acc: 90.77%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3258, Train Acc: 90.74%, Val Loss: 0.3163, Val Acc: 91.16%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   504 |      79.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,640 |      82.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   118 |      85.95%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 11,296 |      81.76%

Compression Ratio (pruning only): 5.48x
Test Accuracy after re-training: 91.75%
Accuracy drop after pruning and re-training: 7.04%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 44
Layer conv_features.1.weight - Safe pruning rate: 72.0%
Layer conv_features.5.weight - Safe pruning rate: 79.0%
Layer conv_features.9.weight - Safe pruning rate: 82.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 86.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.72 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.6636, Train Acc: 67.58%, Val Loss: 0.6025, Val Acc: 83.07%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4590, Train Acc: 86.88%, Val Loss: 0.3971, Val Acc: 88.90%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3583, Train Acc: 89.60%, Val Loss: 0.3599, Val Acc: 90.09%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3408, Train Acc: 90.04%, Val Loss: 0.3323, Val Acc: 90.55%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3334, Train Acc: 90.41%, Val Loss: 0.3320, Val Acc: 90.47%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3268, Train Acc: 90.52%, Val Loss: 0.3194, Val Acc: 90.78%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3267, Train Acc: 90.59%, Val Loss: 0.3246, Val Acc: 90.56%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3240, Train Acc: 90.68%, Val Loss: 0.3269, Val Acc: 90.79%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3297, Train Acc: 90.49%, Val Loss: 0.3239, Val Acc: 90.53%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3285, Train Acc: 90.41%, Val Loss: 0.3372, Val Acc: 90.54%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   504 |      79.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,640 |      82.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   118 |      85.95%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 11,296 |      81.76%

Compression Ratio (pruning only): 5.48x
Test Accuracy after re-training: 91.53%
Accuracy drop after pruning and re-training: 7.26%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 45
Layer conv_features.1.weight - Safe pruning rate: 74.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 83.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 87.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.74 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.7982, Train Acc: 65.05%, Val Loss: 0.6418, Val Acc: 81.11%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4990, Train Acc: 85.39%, Val Loss: 0.3987, Val Acc: 88.31%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3834, Train Acc: 88.84%, Val Loss: 0.4017, Val Acc: 88.41%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3620, Train Acc: 89.36%, Val Loss: 0.3852, Val Acc: 88.59%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3604, Train Acc: 89.58%, Val Loss: 0.3565, Val Acc: 89.62%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3551, Train Acc: 89.59%, Val Loss: 0.3507, Val Acc: 89.49%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3446, Train Acc: 89.85%, Val Loss: 0.3527, Val Acc: 89.67%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3545, Train Acc: 89.68%, Val Loss: 0.3455, Val Acc: 90.12%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3551, Train Acc: 89.70%, Val Loss: 0.3499, Val Acc: 90.07%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3492, Train Acc: 89.79%, Val Loss: 0.3497, Val Acc: 89.51%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,160 |      83.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   109 |      87.02%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,783 |      82.59%

Compression Ratio (pruning only): 5.74x
Test Accuracy after re-training: 90.66%
Accuracy drop after pruning and re-training: 8.13%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 46
Layer conv_features.1.weight - Safe pruning rate: 74.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 83.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 87.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.74 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.8363, Train Acc: 64.59%, Val Loss: 0.6557, Val Acc: 80.69%
Epoch [2/10], LR: 0.010000, Train Loss: 0.4918, Train Acc: 85.54%, Val Loss: 0.3946, Val Acc: 88.39%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3800, Train Acc: 88.90%, Val Loss: 0.3724, Val Acc: 89.20%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3642, Train Acc: 89.30%, Val Loss: 0.3514, Val Acc: 89.51%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3504, Train Acc: 89.92%, Val Loss: 0.3543, Val Acc: 89.75%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3503, Train Acc: 89.69%, Val Loss: 0.3520, Val Acc: 89.88%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3465, Train Acc: 89.81%, Val Loss: 0.3420, Val Acc: 89.85%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3563, Train Acc: 89.78%, Val Loss: 0.3585, Val Acc: 89.83%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3597, Train Acc: 89.46%, Val Loss: 0.3553, Val Acc: 89.81%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3492, Train Acc: 89.74%, Val Loss: 0.3468, Val Acc: 89.90%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,160 |      83.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   109 |      87.02%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,783 |      82.59%

Compression Ratio (pruning only): 5.74x
Test Accuracy after re-training: 90.46%
Accuracy drop after pruning and re-training: 8.33%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 47
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 83.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 87.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.8192, Train Acc: 64.75%, Val Loss: 0.6294, Val Acc: 81.97%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5019, Train Acc: 85.28%, Val Loss: 0.3931, Val Acc: 88.22%
Epoch [3/10], LR: 0.001000, Train Loss: 0.3775, Train Acc: 88.96%, Val Loss: 0.3858, Val Acc: 88.48%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3654, Train Acc: 89.27%, Val Loss: 0.3463, Val Acc: 89.72%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3528, Train Acc: 89.63%, Val Loss: 0.3497, Val Acc: 89.47%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3519, Train Acc: 89.74%, Val Loss: 0.3488, Val Acc: 89.73%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3536, Train Acc: 89.65%, Val Loss: 0.3389, Val Acc: 90.06%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3520, Train Acc: 89.52%, Val Loss: 0.3517, Val Acc: 89.82%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3456, Train Acc: 89.91%, Val Loss: 0.3643, Val Acc: 89.47%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3464, Train Acc: 89.76%, Val Loss: 0.3495, Val Acc: 89.79%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 8,160 |      83.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   109 |      87.02%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,783 |      82.59%

Compression Ratio (pruning only): 5.74x
Test Accuracy after re-training: 90.53%
Accuracy drop after pruning and re-training: 8.26%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 48
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 84.0%
Layer linear_features.0.weight - Safe pruning rate: 85.0%
Layer linear_features.3.weight - Safe pruning rate: 87.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.8720, Train Acc: 64.47%, Val Loss: 0.6666, Val Acc: 81.28%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5121, Train Acc: 85.09%, Val Loss: 0.4022, Val Acc: 88.17%
Epoch [3/10], LR: 0.001000, Train Loss: 0.4039, Train Acc: 88.28%, Val Loss: 0.3932, Val Acc: 88.46%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3822, Train Acc: 88.86%, Val Loss: 0.3657, Val Acc: 89.03%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3719, Train Acc: 89.12%, Val Loss: 0.3710, Val Acc: 89.03%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3722, Train Acc: 89.06%, Val Loss: 0.3661, Val Acc: 89.29%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3694, Train Acc: 89.21%, Val Loss: 0.3691, Val Acc: 89.56%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3709, Train Acc: 89.12%, Val Loss: 0.3650, Val Acc: 89.02%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3646, Train Acc: 89.27%, Val Loss: 0.3551, Val Acc: 89.63%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3679, Train Acc: 89.01%, Val Loss: 0.3597, Val Acc: 89.58%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,680 |      84.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,512 |      85.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   109 |      87.02%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,303 |      83.36%

Compression Ratio (pruning only): 6.01x
Test Accuracy after re-training: 89.97%
Accuracy drop after pruning and re-training: 8.82%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 49
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 84.0%
Layer linear_features.0.weight - Safe pruning rate: 86.0%
Layer linear_features.3.weight - Safe pruning rate: 88.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 2.0307, Train Acc: 62.03%, Val Loss: 0.7159, Val Acc: 78.33%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5402, Train Acc: 84.19%, Val Loss: 0.4571, Val Acc: 86.81%
Epoch [3/10], LR: 0.001000, Train Loss: 0.4156, Train Acc: 87.74%, Val Loss: 0.4069, Val Acc: 87.97%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3951, Train Acc: 88.32%, Val Loss: 0.3827, Val Acc: 88.45%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3881, Train Acc: 88.58%, Val Loss: 0.3865, Val Acc: 88.82%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3886, Train Acc: 88.68%, Val Loss: 0.3750, Val Acc: 89.12%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3788, Train Acc: 88.83%, Val Loss: 0.3775, Val Acc: 88.73%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3879, Train Acc: 88.60%, Val Loss: 0.3912, Val Acc: 88.60%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3787, Train Acc: 88.98%, Val Loss: 0.3801, Val Acc: 88.81%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3833, Train Acc: 88.50%, Val Loss: 0.3920, Val Acc: 88.93%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,680 |      84.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,411 |      86.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |   101 |      87.98%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,194 |      83.54%

Compression Ratio (pruning only): 6.07x
Test Accuracy after re-training: 89.68%
Accuracy drop after pruning and re-training: 9.11%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 50
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 84.0%
Layer linear_features.0.weight - Safe pruning rate: 86.0%
Layer linear_features.3.weight - Safe pruning rate: 89.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.9542, Train Acc: 62.54%, Val Loss: 0.6984, Val Acc: 79.60%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5362, Train Acc: 84.10%, Val Loss: 0.4126, Val Acc: 88.04%
Epoch [3/10], LR: 0.001000, Train Loss: 0.4114, Train Acc: 87.90%, Val Loss: 0.4063, Val Acc: 88.21%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3838, Train Acc: 88.64%, Val Loss: 0.4094, Val Acc: 87.97%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3775, Train Acc: 88.88%, Val Loss: 0.3734, Val Acc: 88.83%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3718, Train Acc: 89.01%, Val Loss: 0.3745, Val Acc: 89.21%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3771, Train Acc: 88.89%, Val Loss: 0.3737, Val Acc: 89.25%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3748, Train Acc: 88.90%, Val Loss: 0.3658, Val Acc: 89.64%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3708, Train Acc: 88.95%, Val Loss: 0.3736, Val Acc: 89.12%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3753, Train Acc: 88.87%, Val Loss: 0.3727, Val Acc: 89.34%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,680 |      84.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,411 |      86.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    92 |      89.05%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,185 |      83.55%

Compression Ratio (pruning only): 6.08x
Test Accuracy after re-training: 89.99%
Accuracy drop after pruning and re-training: 8.80%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 51
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 84.0%
Layer linear_features.0.weight - Safe pruning rate: 86.0%
Layer linear_features.3.weight - Safe pruning rate: 89.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 1.9690, Train Acc: 62.39%, Val Loss: 0.7043, Val Acc: 78.68%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5352, Train Acc: 84.26%, Val Loss: 0.4123, Val Acc: 88.07%
Epoch [3/10], LR: 0.001000, Train Loss: 0.4024, Train Acc: 88.06%, Val Loss: 0.3917, Val Acc: 88.59%
Epoch [4/10], LR: 0.001000, Train Loss: 0.3886, Train Acc: 88.64%, Val Loss: 0.3823, Val Acc: 89.09%
Epoch [5/10], LR: 0.000100, Train Loss: 0.3802, Train Acc: 88.83%, Val Loss: 0.3609, Val Acc: 88.96%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3784, Train Acc: 89.02%, Val Loss: 0.3731, Val Acc: 89.12%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3799, Train Acc: 88.96%, Val Loss: 0.3758, Val Acc: 88.88%
Epoch [8/10], LR: 0.000010, Train Loss: 0.3710, Train Acc: 89.10%, Val Loss: 0.3787, Val Acc: 89.18%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3775, Train Acc: 88.94%, Val Loss: 0.3648, Val Acc: 89.26%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3689, Train Acc: 88.99%, Val Loss: 0.3840, Val Acc: 88.91%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,680 |      84.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,411 |      86.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    92 |      89.05%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 10,185 |      83.55%

Compression Ratio (pruning only): 6.08x
Test Accuracy after re-training: 89.98%
Accuracy drop after pruning and re-training: 8.81%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 52
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 80.0%
Layer conv_features.9.weight - Safe pruning rate: 85.0%
Layer linear_features.0.weight - Safe pruning rate: 87.0%
Layer linear_features.3.weight - Safe pruning rate: 89.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 2.0359, Train Acc: 60.74%, Val Loss: 0.7799, Val Acc: 77.47%
Epoch [2/10], LR: 0.010000, Train Loss: 0.5568, Train Acc: 83.56%, Val Loss: 0.4590, Val Acc: 86.52%
Epoch [3/10], LR: 0.001000, Train Loss: 0.4285, Train Acc: 87.38%, Val Loss: 0.4049, Val Acc: 88.05%
Epoch [4/10], LR: 0.001000, Train Loss: 0.4036, Train Acc: 88.15%, Val Loss: 0.3837, Val Acc: 88.29%
Epoch [5/10], LR: 0.000100, Train Loss: 0.4015, Train Acc: 88.18%, Val Loss: 0.3881, Val Acc: 88.67%
Epoch [6/10], LR: 0.000100, Train Loss: 0.3912, Train Acc: 88.58%, Val Loss: 0.3953, Val Acc: 88.37%
Epoch [7/10], LR: 0.000010, Train Loss: 0.3950, Train Acc: 88.32%, Val Loss: 0.3890, Val Acc: 88.89%
Epoch [8/10], LR: 0.000010, Train Loss: 0.4010, Train Acc: 88.27%, Val Loss: 0.3898, Val Acc: 88.95%
Epoch [9/10], LR: 0.000001, Train Loss: 0.3856, Train Acc: 88.67%, Val Loss: 0.3863, Val Acc: 88.68%
Epoch [10/10], LR: 0.000001, Train Loss: 0.3957, Train Acc: 88.34%, Val Loss: 0.3828, Val Acc: 88.70%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,200 |      85.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,310 |      87.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    92 |      89.05%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 9,604 |      84.49%

Compression Ratio (pruning only): 6.45x
Test Accuracy after re-training: 89.39%
Accuracy drop after pruning and re-training: 9.40%
Final model is acceptable, try bigger accuracy_drop_tolerance
Current layer accuracy dropdown tolerance: 53
Layer conv_features.1.weight - Safe pruning rate: 75.0%
Layer conv_features.5.weight - Safe pruning rate: 82.0%
Layer conv_features.9.weight - Safe pruning rate: 85.0%
Layer linear_features.0.weight - Safe pruning rate: 87.0%
Layer linear_features.3.weight - Safe pruning rate: 90.0%
Pruning conv_features.1.weight | Shape: (6, 1, 5, 5) | Elements: 150
Adjusting pruning percentage for conv_features.1.weight from 0.75 to 0.5733333333333333 to keep at least 20 parameters
Pruning conv_features.5.weight | Shape: (16, 6, 5, 5) | Elements: 2400
Pruning conv_features.9.weight | Shape: (120, 16, 5, 5) | Elements: 48000
Pruning linear_features.0.weight | Shape: (84, 120) | Elements: 10080
Pruning linear_features.3.weight | Shape: (10, 84) | Elements: 840

Pruning Stage 3: Re-training the Pruned Model
Epoch [1/10], LR: 0.010000, Train Loss: 2.1955, Train Acc: 56.99%, Val Loss: 0.8716, Val Acc: 70.92%
Epoch [2/10], LR: 0.010000, Train Loss: 0.6730, Train Acc: 76.09%, Val Loss: 0.5563, Val Acc: 79.96%
Epoch [3/10], LR: 0.001000, Train Loss: 0.5155, Train Acc: 81.30%, Val Loss: 0.5104, Val Acc: 81.31%
Epoch [4/10], LR: 0.001000, Train Loss: 0.4902, Train Acc: 82.10%, Val Loss: 0.4956, Val Acc: 82.09%
Epoch [5/10], LR: 0.000100, Train Loss: 0.4871, Train Acc: 82.58%, Val Loss: 0.4785, Val Acc: 82.48%
Epoch [6/10], LR: 0.000100, Train Loss: 0.4830, Train Acc: 82.48%, Val Loss: 0.4803, Val Acc: 82.54%
Epoch [7/10], LR: 0.000010, Train Loss: 0.4803, Train Acc: 82.22%, Val Loss: 0.4868, Val Acc: 82.47%
Epoch [8/10], LR: 0.000010, Train Loss: 0.4737, Train Acc: 82.72%, Val Loss: 0.4791, Val Acc: 82.63%
Epoch [9/10], LR: 0.000001, Train Loss: 0.4784, Train Acc: 82.70%, Val Loss: 0.4867, Val Acc: 82.42%
Epoch [10/10], LR: 0.000001, Train Loss: 0.4753, Train Acc: 82.76%, Val Loss: 0.4715, Val Acc: 82.92%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   432 |      82.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,200 |      85.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,310 |      87.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    84 |      90.00%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 9,548 |      84.58%

Compression Ratio (pruning only): 6.49x
Test Accuracy after re-training: 83.16%
Accuracy drop after pruning and re-training: 15.62%
end.
Final Test Accuracy: 89.44%

Pruning Stage 5: Final Retraining
Epoch [1/200], LR: 0.010000, Train Loss: 0.3279, Train Acc: 90.32%, Val Loss: 0.2905, Val Acc: 91.46%
Epoch [2/200], LR: 0.010000, Train Loss: 0.2379, Train Acc: 92.80%, Val Loss: 0.2156, Val Acc: 93.45%
Epoch [3/200], LR: 0.010000, Train Loss: 0.1988, Train Acc: 93.85%, Val Loss: 0.1869, Val Acc: 94.15%
Epoch [4/200], LR: 0.010000, Train Loss: 0.1743, Train Acc: 94.52%, Val Loss: 0.1822, Val Acc: 94.37%
Epoch [5/200], LR: 0.010000, Train Loss: 0.1638, Train Acc: 94.99%, Val Loss: 0.1681, Val Acc: 94.72%
Epoch [6/200], LR: 0.010000, Train Loss: 0.1493, Train Acc: 95.40%, Val Loss: 0.1549, Val Acc: 95.28%
Epoch [7/200], LR: 0.010000, Train Loss: 0.1380, Train Acc: 95.67%, Val Loss: 0.1486, Val Acc: 95.56%
Epoch [8/200], LR: 0.010000, Train Loss: 0.1381, Train Acc: 95.68%, Val Loss: 0.1435, Val Acc: 95.49%
Epoch [9/200], LR: 0.010000, Train Loss: 0.1300, Train Acc: 95.85%, Val Loss: 0.1314, Val Acc: 95.80%
Epoch [10/200], LR: 0.010000, Train Loss: 0.1248, Train Acc: 96.09%, Val Loss: 0.1388, Val Acc: 95.70%
Epoch [11/200], LR: 0.010000, Train Loss: 0.1227, Train Acc: 96.17%, Val Loss: 0.1234, Val Acc: 96.32%
Epoch [12/200], LR: 0.010000, Train Loss: 0.1191, Train Acc: 96.23%, Val Loss: 0.1335, Val Acc: 95.87%
Epoch [13/200], LR: 0.010000, Train Loss: 0.1166, Train Acc: 96.43%, Val Loss: 0.1155, Val Acc: 96.42%
Epoch [14/200], LR: 0.010000, Train Loss: 0.1154, Train Acc: 96.43%, Val Loss: 0.1270, Val Acc: 96.10%
Epoch [15/200], LR: 0.010000, Train Loss: 0.1121, Train Acc: 96.47%, Val Loss: 0.1062, Val Acc: 96.63%
Epoch [16/200], LR: 0.010000, Train Loss: 0.1115, Train Acc: 96.52%, Val Loss: 0.1116, Val Acc: 96.49%
Epoch [17/200], LR: 0.010000, Train Loss: 0.1076, Train Acc: 96.66%, Val Loss: 0.1205, Val Acc: 96.42%
Epoch [18/200], LR: 0.010000, Train Loss: 0.1056, Train Acc: 96.56%, Val Loss: 0.1165, Val Acc: 96.45%
Epoch [19/200], LR: 0.010000, Train Loss: 0.1068, Train Acc: 96.69%, Val Loss: 0.1307, Val Acc: 96.03%
Epoch [20/200], LR: 0.010000, Train Loss: 0.1046, Train Acc: 96.75%, Val Loss: 0.1070, Val Acc: 96.70%
Epoch [21/200], LR: 0.010000, Train Loss: 0.1039, Train Acc: 96.73%, Val Loss: 0.1003, Val Acc: 97.08%
Epoch [22/200], LR: 0.010000, Train Loss: 0.1004, Train Acc: 96.81%, Val Loss: 0.1062, Val Acc: 96.78%
Epoch [23/200], LR: 0.010000, Train Loss: 0.1000, Train Acc: 96.86%, Val Loss: 0.0983, Val Acc: 96.90%
Epoch [24/200], LR: 0.010000, Train Loss: 0.1009, Train Acc: 96.85%, Val Loss: 0.1058, Val Acc: 96.76%
Epoch [25/200], LR: 0.010000, Train Loss: 0.0996, Train Acc: 96.86%, Val Loss: 0.1022, Val Acc: 96.83%
Epoch [26/200], LR: 0.010000, Train Loss: 0.0981, Train Acc: 96.94%, Val Loss: 0.1033, Val Acc: 96.79%
Epoch [27/200], LR: 0.010000, Train Loss: 0.0995, Train Acc: 96.86%, Val Loss: 0.1038, Val Acc: 96.73%
Epoch [28/200], LR: 0.010000, Train Loss: 0.0956, Train Acc: 96.96%, Val Loss: 0.1165, Val Acc: 96.41%
Epoch [29/200], LR: 0.010000, Train Loss: 0.0976, Train Acc: 96.93%, Val Loss: 0.0988, Val Acc: 96.90%
Epoch [30/200], LR: 0.010000, Train Loss: 0.0942, Train Acc: 97.01%, Val Loss: 0.1072, Val Acc: 96.79%
Epoch [31/200], LR: 0.010000, Train Loss: 0.0951, Train Acc: 96.99%, Val Loss: 0.0974, Val Acc: 96.97%
Epoch [32/200], LR: 0.010000, Train Loss: 0.0974, Train Acc: 97.00%, Val Loss: 0.0933, Val Acc: 97.07%
Epoch [33/200], LR: 0.010000, Train Loss: 0.0946, Train Acc: 97.04%, Val Loss: 0.1000, Val Acc: 96.88%
Epoch [34/200], LR: 0.010000, Train Loss: 0.0914, Train Acc: 97.04%, Val Loss: 0.0920, Val Acc: 97.12%
Epoch [35/200], LR: 0.010000, Train Loss: 0.0943, Train Acc: 97.05%, Val Loss: 0.0941, Val Acc: 97.10%
Epoch [36/200], LR: 0.010000, Train Loss: 0.0937, Train Acc: 97.02%, Val Loss: 0.0977, Val Acc: 97.03%
Epoch [37/200], LR: 0.010000, Train Loss: 0.0923, Train Acc: 97.08%, Val Loss: 0.1015, Val Acc: 96.76%
Epoch [38/200], LR: 0.010000, Train Loss: 0.0917, Train Acc: 97.09%, Val Loss: 0.0980, Val Acc: 96.93%
Epoch [39/200], LR: 0.010000, Train Loss: 0.0914, Train Acc: 97.12%, Val Loss: 0.0973, Val Acc: 96.78%
Epoch [40/200], LR: 0.010000, Train Loss: 0.0905, Train Acc: 97.12%, Val Loss: 0.1009, Val Acc: 96.83%
Epoch [41/200], LR: 0.001000, Train Loss: 0.0842, Train Acc: 97.17%, Val Loss: 0.0897, Val Acc: 97.33%
Epoch [42/200], LR: 0.001000, Train Loss: 0.0791, Train Acc: 97.54%, Val Loss: 0.0833, Val Acc: 97.55%
Epoch [43/200], LR: 0.001000, Train Loss: 0.0795, Train Acc: 97.47%, Val Loss: 0.0826, Val Acc: 97.54%
Epoch [44/200], LR: 0.001000, Train Loss: 0.0756, Train Acc: 97.59%, Val Loss: 0.0795, Val Acc: 97.67%
Epoch [45/200], LR: 0.001000, Train Loss: 0.0793, Train Acc: 97.44%, Val Loss: 0.0905, Val Acc: 97.28%
Epoch [46/200], LR: 0.001000, Train Loss: 0.0786, Train Acc: 97.54%, Val Loss: 0.0801, Val Acc: 97.50%
Epoch [47/200], LR: 0.001000, Train Loss: 0.0788, Train Acc: 97.44%, Val Loss: 0.0799, Val Acc: 97.66%
Epoch [48/200], LR: 0.001000, Train Loss: 0.0749, Train Acc: 97.64%, Val Loss: 0.0914, Val Acc: 97.06%
Epoch [49/200], LR: 0.001000, Train Loss: 0.0792, Train Acc: 97.46%, Val Loss: 0.0853, Val Acc: 97.32%
Epoch [50/200], LR: 0.001000, Train Loss: 0.0756, Train Acc: 97.69%, Val Loss: 0.0833, Val Acc: 97.38%
Epoch [51/200], LR: 0.001000, Train Loss: 0.0771, Train Acc: 97.55%, Val Loss: 0.0813, Val Acc: 97.43%
Epoch [52/200], LR: 0.001000, Train Loss: 0.0790, Train Acc: 97.50%, Val Loss: 0.0780, Val Acc: 97.47%
Epoch [53/200], LR: 0.001000, Train Loss: 0.0734, Train Acc: 97.67%, Val Loss: 0.0790, Val Acc: 97.57%
Epoch [54/200], LR: 0.001000, Train Loss: 0.0751, Train Acc: 97.60%, Val Loss: 0.0851, Val Acc: 97.42%
Epoch [55/200], LR: 0.001000, Train Loss: 0.0743, Train Acc: 97.64%, Val Loss: 0.0807, Val Acc: 97.56%
Epoch [56/200], LR: 0.001000, Train Loss: 0.0745, Train Acc: 97.61%, Val Loss: 0.0800, Val Acc: 97.47%
Epoch [57/200], LR: 0.001000, Train Loss: 0.0776, Train Acc: 97.60%, Val Loss: 0.0836, Val Acc: 97.38%
Epoch [58/200], LR: 0.001000, Train Loss: 0.0788, Train Acc: 97.51%, Val Loss: 0.0847, Val Acc: 97.50%
Epoch [59/200], LR: 0.001000, Train Loss: 0.0759, Train Acc: 97.61%, Val Loss: 0.0822, Val Acc: 97.38%
Epoch [60/200], LR: 0.001000, Train Loss: 0.0753, Train Acc: 97.61%, Val Loss: 0.0787, Val Acc: 97.58%
Epoch [61/200], LR: 0.001000, Train Loss: 0.0761, Train Acc: 97.64%, Val Loss: 0.0816, Val Acc: 97.53%
Epoch [62/200], LR: 0.001000, Train Loss: 0.0719, Train Acc: 97.72%, Val Loss: 0.0854, Val Acc: 97.39%
Epoch [63/200], LR: 0.001000, Train Loss: 0.0765, Train Acc: 97.54%, Val Loss: 0.0805, Val Acc: 97.55%
Epoch [64/200], LR: 0.001000, Train Loss: 0.0766, Train Acc: 97.58%, Val Loss: 0.0790, Val Acc: 97.60%
Epoch [65/200], LR: 0.001000, Train Loss: 0.0749, Train Acc: 97.63%, Val Loss: 0.0768, Val Acc: 97.54%
Epoch [66/200], LR: 0.001000, Train Loss: 0.0761, Train Acc: 97.58%, Val Loss: 0.0860, Val Acc: 97.40%
Epoch [67/200], LR: 0.001000, Train Loss: 0.0762, Train Acc: 97.63%, Val Loss: 0.0788, Val Acc: 97.52%
Epoch [68/200], LR: 0.001000, Train Loss: 0.0763, Train Acc: 97.60%, Val Loss: 0.0837, Val Acc: 97.48%
Epoch [69/200], LR: 0.001000, Train Loss: 0.0721, Train Acc: 97.65%, Val Loss: 0.0811, Val Acc: 97.47%
Epoch [70/200], LR: 0.001000, Train Loss: 0.0735, Train Acc: 97.64%, Val Loss: 0.0799, Val Acc: 97.46%
Epoch [71/200], LR: 0.001000, Train Loss: 0.0740, Train Acc: 97.58%, Val Loss: 0.0753, Val Acc: 97.68%
Epoch [72/200], LR: 0.001000, Train Loss: 0.0740, Train Acc: 97.70%, Val Loss: 0.0761, Val Acc: 97.58%
Epoch [73/200], LR: 0.001000, Train Loss: 0.0752, Train Acc: 97.57%, Val Loss: 0.0752, Val Acc: 97.58%
Epoch [74/200], LR: 0.001000, Train Loss: 0.0755, Train Acc: 97.60%, Val Loss: 0.0751, Val Acc: 97.47%
Epoch [75/200], LR: 0.001000, Train Loss: 0.0729, Train Acc: 97.69%, Val Loss: 0.0797, Val Acc: 97.58%
Epoch [76/200], LR: 0.001000, Train Loss: 0.0746, Train Acc: 97.66%, Val Loss: 0.0791, Val Acc: 97.58%
Epoch [77/200], LR: 0.001000, Train Loss: 0.0719, Train Acc: 97.71%, Val Loss: 0.0813, Val Acc: 97.47%
Epoch [78/200], LR: 0.001000, Train Loss: 0.0730, Train Acc: 97.66%, Val Loss: 0.0829, Val Acc: 97.38%
Epoch [79/200], LR: 0.001000, Train Loss: 0.0748, Train Acc: 97.60%, Val Loss: 0.0846, Val Acc: 97.30%
Epoch [80/200], LR: 0.001000, Train Loss: 0.0740, Train Acc: 97.67%, Val Loss: 0.0814, Val Acc: 97.46%
Epoch [81/200], LR: 0.000100, Train Loss: 0.0718, Train Acc: 97.69%, Val Loss: 0.0824, Val Acc: 97.49%
Epoch [82/200], LR: 0.000100, Train Loss: 0.0706, Train Acc: 97.77%, Val Loss: 0.0735, Val Acc: 97.63%
Epoch [83/200], LR: 0.000100, Train Loss: 0.0699, Train Acc: 97.64%, Val Loss: 0.0777, Val Acc: 97.56%
Epoch [84/200], LR: 0.000100, Train Loss: 0.0723, Train Acc: 97.69%, Val Loss: 0.0784, Val Acc: 97.62%
Epoch [85/200], LR: 0.000100, Train Loss: 0.0680, Train Acc: 97.88%, Val Loss: 0.0719, Val Acc: 97.66%
Epoch [86/200], LR: 0.000100, Train Loss: 0.0688, Train Acc: 97.82%, Val Loss: 0.0746, Val Acc: 97.63%
Epoch [87/200], LR: 0.000100, Train Loss: 0.0713, Train Acc: 97.76%, Val Loss: 0.0824, Val Acc: 97.47%
Epoch [88/200], LR: 0.000100, Train Loss: 0.0701, Train Acc: 97.81%, Val Loss: 0.0774, Val Acc: 97.55%
Epoch [89/200], LR: 0.000100, Train Loss: 0.0732, Train Acc: 97.73%, Val Loss: 0.0745, Val Acc: 97.68%
Epoch [90/200], LR: 0.000100, Train Loss: 0.0713, Train Acc: 97.71%, Val Loss: 0.0744, Val Acc: 97.80%
Epoch [91/200], LR: 0.000100, Train Loss: 0.0725, Train Acc: 97.67%, Val Loss: 0.0794, Val Acc: 97.75%
Epoch [92/200], LR: 0.000100, Train Loss: 0.0708, Train Acc: 97.81%, Val Loss: 0.0816, Val Acc: 97.43%
Epoch [93/200], LR: 0.000100, Train Loss: 0.0732, Train Acc: 97.62%, Val Loss: 0.0765, Val Acc: 97.70%
Epoch [94/200], LR: 0.000100, Train Loss: 0.0699, Train Acc: 97.70%, Val Loss: 0.0770, Val Acc: 97.58%
Epoch [95/200], LR: 0.000100, Train Loss: 0.0696, Train Acc: 97.73%, Val Loss: 0.0783, Val Acc: 97.77%
Epoch [96/200], LR: 0.000100, Train Loss: 0.0720, Train Acc: 97.76%, Val Loss: 0.0760, Val Acc: 97.56%
Epoch [97/200], LR: 0.000100, Train Loss: 0.0696, Train Acc: 97.74%, Val Loss: 0.0803, Val Acc: 97.55%
Epoch [98/200], LR: 0.000100, Train Loss: 0.0691, Train Acc: 97.82%, Val Loss: 0.0792, Val Acc: 97.53%
Epoch [99/200], LR: 0.000100, Train Loss: 0.0689, Train Acc: 97.73%, Val Loss: 0.0729, Val Acc: 97.85%
Epoch [100/200], LR: 0.000100, Train Loss: 0.0701, Train Acc: 97.76%, Val Loss: 0.0772, Val Acc: 97.56%
Epoch [101/200], LR: 0.000100, Train Loss: 0.0719, Train Acc: 97.71%, Val Loss: 0.0751, Val Acc: 97.70%
Epoch [102/200], LR: 0.000100, Train Loss: 0.0717, Train Acc: 97.73%, Val Loss: 0.0752, Val Acc: 97.68%
Epoch [103/200], LR: 0.000100, Train Loss: 0.0677, Train Acc: 97.83%, Val Loss: 0.0735, Val Acc: 97.78%
Epoch [104/200], LR: 0.000100, Train Loss: 0.0710, Train Acc: 97.76%, Val Loss: 0.0801, Val Acc: 97.52%
Epoch [105/200], LR: 0.000100, Train Loss: 0.0734, Train Acc: 97.62%, Val Loss: 0.0791, Val Acc: 97.65%
Epoch [106/200], LR: 0.000100, Train Loss: 0.0699, Train Acc: 97.77%, Val Loss: 0.0769, Val Acc: 97.60%
Epoch [107/200], LR: 0.000100, Train Loss: 0.0695, Train Acc: 97.81%, Val Loss: 0.0773, Val Acc: 97.64%
Epoch [108/200], LR: 0.000100, Train Loss: 0.0711, Train Acc: 97.72%, Val Loss: 0.0759, Val Acc: 97.61%
Epoch [109/200], LR: 0.000100, Train Loss: 0.0712, Train Acc: 97.71%, Val Loss: 0.0724, Val Acc: 97.78%
Epoch [110/200], LR: 0.000100, Train Loss: 0.0705, Train Acc: 97.75%, Val Loss: 0.0774, Val Acc: 97.70%
Epoch [111/200], LR: 0.000100, Train Loss: 0.0732, Train Acc: 97.67%, Val Loss: 0.0775, Val Acc: 97.61%
Epoch [112/200], LR: 0.000100, Train Loss: 0.0714, Train Acc: 97.71%, Val Loss: 0.0746, Val Acc: 97.57%
Epoch [113/200], LR: 0.000100, Train Loss: 0.0723, Train Acc: 97.66%, Val Loss: 0.0785, Val Acc: 97.47%
Epoch [114/200], LR: 0.000100, Train Loss: 0.0714, Train Acc: 97.68%, Val Loss: 0.0786, Val Acc: 97.64%
Epoch [115/200], LR: 0.000100, Train Loss: 0.0728, Train Acc: 97.70%, Val Loss: 0.0741, Val Acc: 97.64%
Epoch [116/200], LR: 0.000100, Train Loss: 0.0704, Train Acc: 97.73%, Val Loss: 0.0766, Val Acc: 97.67%
Epoch [117/200], LR: 0.000100, Train Loss: 0.0677, Train Acc: 97.82%, Val Loss: 0.0761, Val Acc: 97.61%
Epoch [118/200], LR: 0.000100, Train Loss: 0.0697, Train Acc: 97.77%, Val Loss: 0.0820, Val Acc: 97.46%
Epoch [119/200], LR: 0.000100, Train Loss: 0.0712, Train Acc: 97.71%, Val Loss: 0.0769, Val Acc: 97.53%
Epoch [120/200], LR: 0.000100, Train Loss: 0.0711, Train Acc: 97.76%, Val Loss: 0.0769, Val Acc: 97.69%
Epoch [121/200], LR: 0.000010, Train Loss: 0.0693, Train Acc: 97.78%, Val Loss: 0.0769, Val Acc: 97.64%
Epoch [122/200], LR: 0.000010, Train Loss: 0.0707, Train Acc: 97.78%, Val Loss: 0.0699, Val Acc: 97.88%
Epoch [123/200], LR: 0.000010, Train Loss: 0.0698, Train Acc: 97.78%, Val Loss: 0.0743, Val Acc: 97.72%
Epoch [124/200], LR: 0.000010, Train Loss: 0.0706, Train Acc: 97.70%, Val Loss: 0.0792, Val Acc: 97.65%
Epoch [125/200], LR: 0.000010, Train Loss: 0.0719, Train Acc: 97.67%, Val Loss: 0.0765, Val Acc: 97.80%
Epoch [126/200], LR: 0.000010, Train Loss: 0.0730, Train Acc: 97.65%, Val Loss: 0.0723, Val Acc: 97.79%
Epoch [127/200], LR: 0.000010, Train Loss: 0.0710, Train Acc: 97.66%, Val Loss: 0.0777, Val Acc: 97.56%
Epoch [128/200], LR: 0.000010, Train Loss: 0.0713, Train Acc: 97.73%, Val Loss: 0.0793, Val Acc: 97.60%
Epoch [129/200], LR: 0.000010, Train Loss: 0.0704, Train Acc: 97.74%, Val Loss: 0.0807, Val Acc: 97.44%
Epoch [130/200], LR: 0.000010, Train Loss: 0.0694, Train Acc: 97.77%, Val Loss: 0.0731, Val Acc: 97.83%
Epoch [131/200], LR: 0.000010, Train Loss: 0.0701, Train Acc: 97.66%, Val Loss: 0.0769, Val Acc: 97.57%
Epoch [132/200], LR: 0.000010, Train Loss: 0.0692, Train Acc: 97.80%, Val Loss: 0.0742, Val Acc: 97.64%
Epoch [133/200], LR: 0.000010, Train Loss: 0.0714, Train Acc: 97.70%, Val Loss: 0.0761, Val Acc: 97.63%
Epoch [134/200], LR: 0.000010, Train Loss: 0.0715, Train Acc: 97.76%, Val Loss: 0.0783, Val Acc: 97.56%
Epoch [135/200], LR: 0.000010, Train Loss: 0.0707, Train Acc: 97.75%, Val Loss: 0.0832, Val Acc: 97.53%
Epoch [136/200], LR: 0.000010, Train Loss: 0.0722, Train Acc: 97.70%, Val Loss: 0.0827, Val Acc: 97.35%
Epoch [137/200], LR: 0.000010, Train Loss: 0.0710, Train Acc: 97.80%, Val Loss: 0.0747, Val Acc: 97.67%
Epoch [138/200], LR: 0.000010, Train Loss: 0.0711, Train Acc: 97.72%, Val Loss: 0.0756, Val Acc: 97.56%
Epoch [139/200], LR: 0.000010, Train Loss: 0.0711, Train Acc: 97.64%, Val Loss: 0.0791, Val Acc: 97.53%
Epoch [140/200], LR: 0.000010, Train Loss: 0.0716, Train Acc: 97.68%, Val Loss: 0.0781, Val Acc: 97.58%
Epoch [141/200], LR: 0.000010, Train Loss: 0.0719, Train Acc: 97.66%, Val Loss: 0.0741, Val Acc: 97.62%
Epoch [142/200], LR: 0.000010, Train Loss: 0.0709, Train Acc: 97.80%, Val Loss: 0.0747, Val Acc: 97.56%
Epoch [143/200], LR: 0.000010, Train Loss: 0.0695, Train Acc: 97.76%, Val Loss: 0.0731, Val Acc: 97.83%
Epoch [144/200], LR: 0.000010, Train Loss: 0.0701, Train Acc: 97.69%, Val Loss: 0.0779, Val Acc: 97.58%
Epoch [145/200], LR: 0.000010, Train Loss: 0.0696, Train Acc: 97.75%, Val Loss: 0.0811, Val Acc: 97.50%
Epoch [146/200], LR: 0.000010, Train Loss: 0.0705, Train Acc: 97.71%, Val Loss: 0.0734, Val Acc: 97.63%
Epoch [147/200], LR: 0.000010, Train Loss: 0.0688, Train Acc: 97.76%, Val Loss: 0.0754, Val Acc: 97.67%
Epoch [148/200], LR: 0.000010, Train Loss: 0.0689, Train Acc: 97.82%, Val Loss: 0.0763, Val Acc: 97.58%
Epoch [149/200], LR: 0.000010, Train Loss: 0.0686, Train Acc: 97.79%, Val Loss: 0.0798, Val Acc: 97.52%
Epoch [150/200], LR: 0.000010, Train Loss: 0.0676, Train Acc: 97.83%, Val Loss: 0.0860, Val Acc: 97.30%
Epoch [151/200], LR: 0.000010, Train Loss: 0.0708, Train Acc: 97.67%, Val Loss: 0.0752, Val Acc: 97.69%
Epoch [152/200], LR: 0.000010, Train Loss: 0.0692, Train Acc: 97.72%, Val Loss: 0.0742, Val Acc: 97.70%
Epoch [153/200], LR: 0.000010, Train Loss: 0.0697, Train Acc: 97.79%, Val Loss: 0.0807, Val Acc: 97.52%
Epoch [154/200], LR: 0.000010, Train Loss: 0.0722, Train Acc: 97.67%, Val Loss: 0.0751, Val Acc: 97.69%
Epoch [155/200], LR: 0.000010, Train Loss: 0.0701, Train Acc: 97.71%, Val Loss: 0.0756, Val Acc: 97.66%
Epoch [156/200], LR: 0.000010, Train Loss: 0.0705, Train Acc: 97.76%, Val Loss: 0.0783, Val Acc: 97.51%
Epoch [157/200], LR: 0.000010, Train Loss: 0.0720, Train Acc: 97.74%, Val Loss: 0.0778, Val Acc: 97.70%
Epoch [158/200], LR: 0.000010, Train Loss: 0.0704, Train Acc: 97.76%, Val Loss: 0.0812, Val Acc: 97.58%
Epoch [159/200], LR: 0.000010, Train Loss: 0.0702, Train Acc: 97.71%, Val Loss: 0.0828, Val Acc: 97.33%
Epoch [160/200], LR: 0.000010, Train Loss: 0.0708, Train Acc: 97.71%, Val Loss: 0.0803, Val Acc: 97.46%
Epoch [161/200], LR: 0.000001, Train Loss: 0.0701, Train Acc: 97.68%, Val Loss: 0.0758, Val Acc: 97.60%
Epoch [162/200], LR: 0.000001, Train Loss: 0.0728, Train Acc: 97.65%, Val Loss: 0.0739, Val Acc: 97.64%
Epoch [163/200], LR: 0.000001, Train Loss: 0.0671, Train Acc: 97.82%, Val Loss: 0.0771, Val Acc: 97.64%
Epoch [164/200], LR: 0.000001, Train Loss: 0.0693, Train Acc: 97.79%, Val Loss: 0.0803, Val Acc: 97.57%
Epoch [165/200], LR: 0.000001, Train Loss: 0.0699, Train Acc: 97.75%, Val Loss: 0.0785, Val Acc: 97.64%
Epoch [166/200], LR: 0.000001, Train Loss: 0.0726, Train Acc: 97.70%, Val Loss: 0.0755, Val Acc: 97.55%
Epoch [167/200], LR: 0.000001, Train Loss: 0.0695, Train Acc: 97.74%, Val Loss: 0.0831, Val Acc: 97.38%
Epoch [168/200], LR: 0.000001, Train Loss: 0.0692, Train Acc: 97.80%, Val Loss: 0.0759, Val Acc: 97.55%
Epoch [169/200], LR: 0.000001, Train Loss: 0.0685, Train Acc: 97.86%, Val Loss: 0.0754, Val Acc: 97.58%
Epoch [170/200], LR: 0.000001, Train Loss: 0.0703, Train Acc: 97.77%, Val Loss: 0.0779, Val Acc: 97.60%
Epoch [171/200], LR: 0.000001, Train Loss: 0.0698, Train Acc: 97.76%, Val Loss: 0.0773, Val Acc: 97.66%
Epoch [172/200], LR: 0.000001, Train Loss: 0.0690, Train Acc: 97.80%, Val Loss: 0.0727, Val Acc: 97.71%
Epoch [173/200], LR: 0.000001, Train Loss: 0.0698, Train Acc: 97.76%, Val Loss: 0.0743, Val Acc: 97.70%
Epoch [174/200], LR: 0.000001, Train Loss: 0.0709, Train Acc: 97.78%, Val Loss: 0.0793, Val Acc: 97.59%
Epoch [175/200], LR: 0.000001, Train Loss: 0.0726, Train Acc: 97.67%, Val Loss: 0.0827, Val Acc: 97.35%
Epoch [176/200], LR: 0.000001, Train Loss: 0.0697, Train Acc: 97.73%, Val Loss: 0.0741, Val Acc: 97.70%
Epoch [177/200], LR: 0.000001, Train Loss: 0.0704, Train Acc: 97.75%, Val Loss: 0.0842, Val Acc: 97.40%
Epoch [178/200], LR: 0.000001, Train Loss: 0.0703, Train Acc: 97.71%, Val Loss: 0.0774, Val Acc: 97.70%
Epoch [179/200], LR: 0.000001, Train Loss: 0.0694, Train Acc: 97.85%, Val Loss: 0.0794, Val Acc: 97.40%
Epoch [180/200], LR: 0.000001, Train Loss: 0.0708, Train Acc: 97.72%, Val Loss: 0.0704, Val Acc: 97.71%
Epoch [181/200], LR: 0.000001, Train Loss: 0.0678, Train Acc: 97.91%, Val Loss: 0.0824, Val Acc: 97.44%
Epoch [182/200], LR: 0.000001, Train Loss: 0.0690, Train Acc: 97.84%, Val Loss: 0.0760, Val Acc: 97.58%
Epoch [183/200], LR: 0.000001, Train Loss: 0.0694, Train Acc: 97.81%, Val Loss: 0.0781, Val Acc: 97.67%
Epoch [184/200], LR: 0.000001, Train Loss: 0.0691, Train Acc: 97.80%, Val Loss: 0.0798, Val Acc: 97.61%
Epoch [185/200], LR: 0.000001, Train Loss: 0.0715, Train Acc: 97.75%, Val Loss: 0.0791, Val Acc: 97.61%
Epoch [186/200], LR: 0.000001, Train Loss: 0.0685, Train Acc: 97.82%, Val Loss: 0.0783, Val Acc: 97.72%
Epoch [187/200], LR: 0.000001, Train Loss: 0.0713, Train Acc: 97.71%, Val Loss: 0.0798, Val Acc: 97.51%
Epoch [188/200], LR: 0.000001, Train Loss: 0.0689, Train Acc: 97.84%, Val Loss: 0.0697, Val Acc: 97.83%
Epoch [189/200], LR: 0.000001, Train Loss: 0.0695, Train Acc: 97.76%, Val Loss: 0.0791, Val Acc: 97.45%
Epoch [190/200], LR: 0.000001, Train Loss: 0.0681, Train Acc: 97.79%, Val Loss: 0.0844, Val Acc: 97.38%
Epoch [191/200], LR: 0.000001, Train Loss: 0.0697, Train Acc: 97.77%, Val Loss: 0.0757, Val Acc: 97.63%
Epoch [192/200], LR: 0.000001, Train Loss: 0.0697, Train Acc: 97.73%, Val Loss: 0.0745, Val Acc: 97.53%
Epoch [193/200], LR: 0.000001, Train Loss: 0.0690, Train Acc: 97.82%, Val Loss: 0.0777, Val Acc: 97.50%
Epoch [194/200], LR: 0.000001, Train Loss: 0.0684, Train Acc: 97.82%, Val Loss: 0.0745, Val Acc: 97.72%
Epoch [195/200], LR: 0.000001, Train Loss: 0.0708, Train Acc: 97.75%, Val Loss: 0.0808, Val Acc: 97.42%
Epoch [196/200], LR: 0.000001, Train Loss: 0.0727, Train Acc: 97.73%, Val Loss: 0.0806, Val Acc: 97.42%
Epoch [197/200], LR: 0.000001, Train Loss: 0.0692, Train Acc: 97.76%, Val Loss: 0.0796, Val Acc: 97.63%
Epoch [198/200], LR: 0.000001, Train Loss: 0.0693, Train Acc: 97.73%, Val Loss: 0.0729, Val Acc: 97.62%
Epoch [199/200], LR: 0.000001, Train Loss: 0.0696, Train Acc: 97.82%, Val Loss: 0.0795, Val Acc: 97.51%
Epoch [200/200], LR: 0.000001, Train Loss: 0.0711, Train Acc: 97.71%, Val Loss: 0.0768, Val Acc: 97.72%
Final Test Accuracy: 97.78%
Parameter Path                                                                           | Layer Type                       | Param Count | Non-zero | Sparsity (%)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
conv_features.1.weight                                                                   | QuantConv2d                      |    150 |    64 |      57.33%
conv_features.2.weight                                                                   | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.2.bias                                                                     | BatchNorm2d                      |      6 |     6 |       0.00%
conv_features.3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.5.weight                                                                   | QuantConv2d                      |  2,400 |   480 |      80.00%
conv_features.6.weight                                                                   | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.6.bias                                                                     | BatchNorm2d                      |     16 |    16 |       0.00%
conv_features.7.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value   | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
conv_features.9.weight                                                                   | QuantConv2d                      | 48,000 | 7,200 |      85.00%
conv_features.10.weight                                                                  | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.10.bias                                                                    | BatchNorm2d                      |    120 |   120 |       0.00%
conv_features.11.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value  | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.0.weight                                                                 | QuantLinear                      | 10,080 | 1,310 |      87.00%
linear_features.1.weight                                                                 | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.1.bias                                                                   | BatchNorm1d                      |     84 |    84 |       0.00%
linear_features.2.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterFromRuntimeStatsScaling |      1 |     1 |       0.00%
linear_features.3.weight                                                                 | QuantLinear                      |    840 |    92 |      89.05%
linear_features.4.weight                                                                 | TensorNorm                       |      1 |     1 |       0.00%
linear_features.4.bias                                                                   | TensorNorm                       |      1 |     1 |       0.00%
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                    | -                                | 61,928 | 9,604 |      84.49%

Compression Ratio (pruning only): 6.45x
Total Parameters: 61,928, Non-zero Parameters: 61,928