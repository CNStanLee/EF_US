import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import heapq
import math
import os
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR
import copy
import random
from models import get_model
from dataset import get_dataloaders

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import heapq
import math
import os
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR
import copy
import random

class SRPopupTrainer:
    def __init__(self, model_name='2c3f', w=4, a=4, epochs=500, random_seed=1998, 
                 dataset_name='MNIST', batch_size=64, lr=0.01, optimizer='sgd', 
                 momentum=0.9, weight_decay=1e-4, lr_scheduler='plateau', patience=10, 
                 threshold=0.5, clip_weights=True, min_lr=1e-6, model=None, freeze_zeros=False,
                 prune_rate=0.9, prune_layers=None, eta=0.99, is_binary=False,
                 pretrained_path=None):
        """
        SRPopupTrainer åˆå§‹åŒ–
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§° (é»˜è®¤ '2c3f')
            w: æƒé‡ä½å®½ (é»˜è®¤ 4)
            a: æ¿€æ´»ä½å®½ (é»˜è®¤ 4)
            epochs: è®­ç»ƒè½®æ•° (é»˜è®¤ 500)
            random_seed: éšæœºç§å­ (é»˜è®¤ 1998)
            dataset_name: æ•°æ®é›†åç§° (é»˜è®¤ 'MNIST')
            batch_size: æ‰¹å¤§å° (é»˜è®¤ 64)
            lr: åˆå§‹å­¦ä¹ ç‡ (é»˜è®¤ 0.01)
            optimizer: ä¼˜åŒ–å™¨ç±»å‹ ('adam' æˆ– 'sgd') (é»˜è®¤ 'sgd')
            momentum: SGDåŠ¨é‡ (é»˜è®¤ 0.9)
            weight_decay: æƒé‡è¡°å‡ (é»˜è®¤ 1e-4)
            lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ ('plateau', 'step', 'cosine' æˆ– None) (é»˜è®¤ 'plateau')
            patience: æ—©åœ/å­¦ä¹ ç‡é™ä½çš„è€å¿ƒå€¼ (é»˜è®¤ 10)
            threshold: äºŒåˆ†ç±»é˜ˆå€¼ (é»˜è®¤ 0.5)
            clip_weights: æ˜¯å¦è£å‰ªæƒé‡ (é»˜è®¤ True)
            min_lr: æœ€å°å­¦ä¹ ç‡ (é»˜è®¤ 1e-6)
            model: é¢„åˆå§‹åŒ–çš„æ¨¡å‹ (é»˜è®¤ None)
            freeze_zeros: æ˜¯å¦å†»ç»“é›¶æƒé‡ (é»˜è®¤ False)
            prune_rate: å‰ªæç‡ (é»˜è®¤ 0.9)
            prune_layers: æŒ‡å®šè¦å‰ªæçš„å±‚ (Noneè¡¨ç¤ºæ‰€æœ‰å±‚)
            eta: å‰ªææƒé‡çš„åˆå§‹æ©ç å€¼ (é»˜è®¤ 0.99)
            is_binary: æ˜¯å¦ä¸ºäºŒåˆ†ç±»ä»»åŠ¡ (é»˜è®¤ False)
            pretrained_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„ (é»˜è®¤ None)
        """
        # ä¿å­˜æ‰€æœ‰å‚æ•°
        self.model_name = model_name
        self.w = w
        self.a = a
        self.epochs = epochs
        self.random_seed = random_seed
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.lr = lr
        self.optimizer_type = optimizer.lower()
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.lr_scheduler_type = lr_scheduler.lower() if lr_scheduler else None
        self.patience = patience
        self.threshold = threshold
        self.clip_weights = clip_weights
        self.min_lr = min_lr
        self.model = model
        self.freeze_zeros = freeze_zeros
        self.prune_rate = prune_rate
        self.prune_layers = prune_layers
        self.eta = eta
        self.is_binary = is_binary
        self.pretrained_path = pretrained_path
        
        # è®¾ç½®éšæœºç§å­
        self._set_seed()
        
        # è·å–è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # åˆ›å»ºç›®å½•
        self.build_dir = "./build"
        self.model_dir = "./model"
        os.makedirs(self.build_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæä¾›è·¯å¾„ï¼‰
        if self.pretrained_path:
            self.load_pretrained(self.pretrained_path)
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        self._init_dataloaders()
        
        # å‡†å¤‡æ¨¡å‹ï¼šæ·»åŠ æ©ç å‚æ•°
        self.prepare_model()
        
        # ä¼˜åŒ–å™¨ï¼šåªä¼˜åŒ–æ©ç å‚æ•°
        mask_params = [param for name, param in self.model.named_parameters() if 'mask' in name]
        self.optimizer = self._get_optimizer(mask_params)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._get_scheduler()
        
        # æŸå¤±å‡½æ•° - æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©
        if self.is_binary:
            self.criterion = nn.BCEWithLogitsLoss()
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.best_epoch = 0
        self.epochs_no_improve = 0
        self.stopped_early = False
    
    def load_pretrained(self, weight_path):
        if not os.path.exists(weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
        
        state_dict = torch.load(weight_path, map_location=self.device)
        model_state_dict = self.model.state_dict()
        
        # æ›´å®½æ¾çš„åŠ è½½ç­–ç•¥
        matched_state_dict = {}
        for k, v in state_dict.items():
            # å°è¯•ç›´æ¥åŒ¹é…
            if k in model_state_dict and model_state_dict[k].shape == v.shape:
                matched_state_dict[k] = v
                continue
                
            # å°è¯•ç§»é™¤å¯èƒ½çš„æ¨¡å—å‰ç¼€
            new_k = k.replace('module.', '')
            if new_k in model_state_dict and model_state_dict[new_k].shape == v.shape:
                matched_state_dict[new_k] = v
                continue
                
            # å°è¯•æ·»åŠ é‡åŒ–åç¼€ (é‡åŒ–æ¨¡å‹å¯èƒ½æœ‰çš„é¢å¤–å‚æ•°)
            quant_k = k + '.weight'
            if quant_k in model_state_dict and model_state_dict[quant_k].shape == v.shape:
                matched_state_dict[quant_k] = v
                continue
        
        # åŠ è½½åŒ¹é…çš„æƒé‡
        model_state_dict.update(matched_state_dict)
        self.model.load_state_dict(model_state_dict, strict=False)
        print(f"ä» {weight_path} åŠ è½½é¢„è®­ç»ƒæƒé‡, åŒ¹é…äº† {len(matched_state_dict)}/{len(state_dict)} ä¸ªå‚æ•°")

    
    def _set_seed(self):
        """è®¾ç½®å…¨å±€éšæœºç§å­"""
        torch.manual_seed(self.random_seed)
        np.random.seed(self.random_seed)
        random.seed(self.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.random_seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model is None:
            # å¦‚æœæ²¡æœ‰æä¾›é¢„åˆå§‹åŒ–çš„æ¨¡å‹ï¼Œåˆ™åˆ›å»ºæ–°æ¨¡å‹
            self.model = get_model(self.model_name, self.w, self.a)
            print(f"åˆ›å»ºæ–°æ¨¡å‹: {self.model_name}")
        else:
            # ä½¿ç”¨æä¾›çš„æ¨¡å‹
            print(f"ä½¿ç”¨é¢„åˆå§‹åŒ–çš„æ¨¡å‹: {self.model_name}")
        
        self.model.to(self.device)
    
    def _init_dataloaders(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        self.train_loader, self.val_loader, self.test_loader = get_dataloaders(
            dataset_name=self.dataset_name,
            batch_size=self.batch_size
        )
    
    def _get_optimizer(self, params):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.optimizer_type == 'adam':
            return optim.Adam(
                params, 
                lr=self.lr, 
                weight_decay=self.weight_decay
            )
        elif self.optimizer_type == 'sgd':
            return optim.SGD(
                params, 
                lr=self.lr, 
                momentum=self.momentum,
                weight_decay=self.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.optimizer_type}")
    
    def _get_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.lr_scheduler_type:
            return None
            
        if self.lr_scheduler_type == 'plateau':
            return ReduceLROnPlateau(
                self.optimizer, 
                mode='max', 
                factor=0.5,
                patience=self.patience // 2,
                verbose=True,
                min_lr=self.min_lr
            )
        elif self.lr_scheduler_type == 'step':
            return StepLR(
                self.optimizer, 
                step_size=self.epochs // 5, 
                gamma=0.1
            )
        elif self.lr_scheduler_type == 'cosine':
            return CosineAnnealingLR(
                self.optimizer, 
                T_max=self.epochs,
                eta_min=self.min_lr
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.lr_scheduler_type}")
    
    def prepare_model(self):
        """
        å‡†å¤‡æ¨¡å‹ï¼šä¸ºæŒ‡å®šå±‚æ·»åŠ æ©ç å‚æ•°
        """
        # å¦‚æœæœªæŒ‡å®šå‰ªæå±‚ï¼Œé»˜è®¤å‰ªææ‰€æœ‰å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚
        if self.prune_layers is None:
            self.prune_layers = []
            for name, module in self.model.named_modules():
                if isinstance(module, (nn.Linear, nn.Conv2d)):
                    self.prune_layers.append((name, module))
        else:
            # å°†å±‚åç§°æ˜ å°„åˆ°æ¨¡å—å¯¹è±¡
            pruned_modules = []
            for name in self.prune_layers:
                module = self.model
                for part in name.split('.'):
                    module = getattr(module, part)
                pruned_modules.append((name, module))
            self.prune_layers = pruned_modules
        
        # æ·»åŠ æ©ç å‚æ•°å¹¶åˆå§‹åŒ–
        for name, module in self.prune_layers:
            # ä¸ºæ¨¡å—æ·»åŠ æ©ç å‚æ•°
            # ä½¿ç”¨å®‰å…¨çš„å‚æ•°åç§°ï¼Œé¿å…ç‚¹å·
            param_name = f"mask_{name.replace('.', '_')}"
            
            # å¦‚æœå·²ç»æ·»åŠ è¿‡æ©ç ï¼Œè·³è¿‡
            if hasattr(module, param_name):
                continue
                
            # æ·»åŠ æ©ç å‚æ•°
            mask = torch.ones_like(module.weight.data, requires_grad=True)
            module.register_parameter(param_name, nn.Parameter(mask))
            
            # å†»ç»“æƒé‡ï¼ˆåªä¼˜åŒ–æ©ç ï¼‰
            module.weight.requires_grad = False
        
        # ä½¿ç”¨å¹…åº¦å‰ªæåˆå§‹åŒ–æ©ç 
        self.initialize_mask_with_magnitude()
            # æ·»åŠ STEå‡½æ•°
        class BinaryMaskSTE(torch.autograd.Function):
            @staticmethod
            def forward(ctx, mask_param, prune_rate, eta):
                # äºŒå€¼åŒ–æ©ç 
                all_mask_vals = mask_param.view(-1)
                k = int(all_mask_vals.numel() * prune_rate)
                threshold = torch.topk(all_mask_vals, k, largest=False).values[-1]
                binary_mask = (mask_param > threshold).float()
                ctx.save_for_backward(mask_param, binary_mask)
                return binary_mask

            @staticmethod
            def backward(ctx, grad_output):
                mask_param, binary_mask = ctx.saved_tensors
                # ç›´é€šä¼°è®¡å™¨: é€šè¿‡äºŒå€¼åŒ–æ“ä½œçš„æ¢¯åº¦
                grad_input = grad_output.clone()
                # å¯é€‰: æ·»åŠ æ¢¯åº¦è£å‰ª
                grad_input = torch.clamp(grad_input, -1.0, 1.0)
                return grad_input, None, None
        
        self.binary_mask_ste = BinaryMaskSTE.apply
    
    def initialize_mask_with_magnitude(self):
        """
        ä½¿ç”¨å¹…åº¦å‰ªæåˆå§‹åŒ–æ©ç 
        """
        # æ”¶é›†æ‰€æœ‰æƒé‡
        all_weights = []
        for name, module in self.prune_layers:
            all_weights.append(module.weight.data.abs().view(-1))
        
        all_weights = torch.cat(all_weights)
        k = int(all_weights.numel() * self.prune_rate)
        threshold = torch.topk(all_weights, k, largest=False).values[-1]
        
        # åº”ç”¨å¹…åº¦å‰ªæåˆå§‹åŒ–æ©ç 
        for name, module in self.prune_layers:
            # è·å–æ©ç å‚æ•°
            param_name = f"mask_{name.replace('.', '_')}"
            mask_param = getattr(module, param_name)
            
            weight_abs = module.weight.data.abs()
            
            # åˆå§‹åŒ–æ©ç ï¼šä¿ç•™çš„æƒé‡ä¸º1ï¼Œå‰ªæçš„æƒé‡ä¸ºeta
            init_mask = torch.ones_like(mask_param.data)
            init_mask[weight_abs < threshold] = self.eta
            mask_param.data = init_mask
    
    def apply_mask(self):
        """åº”ç”¨å¸¦STEçš„æ©ç åˆ°æƒé‡"""
        for name, module in self.prune_layers:
            param_name = f"mask_{name.replace('.', '_')}"
            mask_param = getattr(module, param_name)
            
            # ä½¿ç”¨STEè·å¾—å¯å¯¼çš„äºŒå€¼æ©ç 
            binary_mask = self.binary_mask_ste(mask_param, self.prune_rate, self.eta)
            
            # åº”ç”¨äºŒå€¼åŒ–æ©ç  - æ³¨æ„: ä¸ä¿®æ”¹æƒé‡æœ¬èº«ï¼Œåªè¿”å›æ©ç åçš„æƒé‡
            return module.weight * binary_mask
    
    def apply_mask_for_layer(self, name, module):
        """ä¸ºå•ä¸ªå±‚åº”ç”¨æ©ç """
        param_name = f"mask_{name.replace('.', '_')}"
        mask_param = getattr(module, param_name)
        
        # ä½¿ç”¨STE
        binary_mask = self.binary_mask_ste(mask_param, self.prune_rate, self.eta)
        
        # è¿”å›æ©ç åçš„æƒé‡ï¼ˆä¸ä¿®æ”¹åŸå§‹æƒé‡ï¼‰
        return module.weight * binary_mask

    def forward(self, x):
        """å¸¦æ©ç çš„å‰å‘ä¼ æ’­ï¼ˆä¸ä¿®æ”¹åŸå§‹æƒé‡ï¼‰"""
        # ä¿å­˜åŸå§‹æƒé‡
        original_weights = {}
        for name, module in self.prune_layers:
            original_weights[name] = module.weight.data.clone()
            
            # ä¸´æ—¶åº”ç”¨æ©ç 
            masked_weight = self.apply_mask_for_layer(name, module)
            module.weight.data = masked_weight
        
        # å‰å‘ä¼ æ’­
        output = self.model(x)
        
        # æ¢å¤åŸå§‹æƒé‡
        for name, module in self.prune_layers:
            module.weight.data = original_weights[name]

        return output
    
    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        """
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # è®¡ç®—æƒé‡äº¤æ¢æ•°é‡
        total_iterations = len(self.train_loader)
        current_iteration = 0
        
        for batch_idx, (inputs, targets) in enumerate(tqdm(self.train_loader, desc=f"Epoch {epoch}")):
            # ...
            # å‰å‘ä¼ æ’­ (ç°åœ¨åœ¨forwardå†…éƒ¨å¤„ç†æ©ç )
            outputs = self.forward(inputs)
            loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # åº”ç”¨çŸ­é™åˆ¶å¼¹å‡º (ä¿æŒåŸæ ·)
            self.apply_short_restriction(epoch, batch_idx, total_iterations)
            
            # æ›´æ–°æ©ç å‚æ•°
            self.optimizer.step()
            # è®¡ç®—å‡†ç¡®ç‡
            if self.is_binary:
                predicted = (torch.sigmoid(outputs) > self.threshold).float()
                correct += (predicted == targets).sum().item()
            else:
                _, predicted = outputs.max(1)
                correct += predicted.eq(targets).sum().item()
            
            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item()
            total += targets.size(0)
            current_iteration += 1
        
        # æ›´æ–°å­¦ä¹ ç‡
        if self.scheduler and not isinstance(self.scheduler, ReduceLROnPlateau):
            self.scheduler.step()
        
        # è®¡ç®—æŒ‡æ ‡
        train_loss = total_loss / len(self.train_loader)
        train_acc = 100. * correct / total
        return train_loss, train_acc
    
    def apply_short_restriction(self, epoch, batch_idx, total_iterations):
        """
        åº”ç”¨çŸ­é™åˆ¶å¼¹å‡ºç­–ç•¥ï¼ˆå®Œæ•´å®ç°ï¼‰
        """
        # è®¡ç®—å½“å‰è¿­ä»£çš„æƒé‡äº¤æ¢ç‡
        t = (epoch - 1) * total_iterations + batch_idx
        t_f = self.epochs * total_iterations
        rate = math.ceil((1 - t / t_f) ** 4)
        
        # åˆ›å»ºä¸´æ—¶æ©ç å‰¯æœ¬ç”¨äºäº¤æ¢
        temp_masks = {}
        for name, module in self.prune_layers:
            param_name = f"mask_{name.replace('.', '_')}"
            mask_param = getattr(module, param_name)
            temp_masks[name] = mask_param.data.clone()
        
        # æ”¶é›†æ‰€æœ‰æ©ç å€¼
        all_masks = []
        all_names = []
        all_indices = []
        
        # éå†æ‰€æœ‰å‰ªæå±‚
        for name, module in self.prune_layers:
            mask_data = temp_masks[name]  # ä½¿ç”¨ä¸´æ—¶å‰¯æœ¬
            
            # å±•å¹³æ©ç å¹¶æ”¶é›†
            flat_mask = mask_data.view(-1)
            all_masks.append(flat_mask)
            
            # è®°å½•åç§°å’Œç´¢å¼•
            num_elements = mask_data.numel()
            all_names.extend([name] * num_elements)
            
            # åˆ›å»ºç´¢å¼•æ˜ å°„
            if len(mask_data.shape) == 1:
                indices = [(i,) for i in range(mask_data.size(0))]
            elif len(mask_data.shape) == 2:
                indices = [(i, j) for i in range(mask_data.size(0)) for j in range(mask_data.size(1))]
            elif len(mask_data.shape) == 4:
                indices = [(i, j, k, l) for i in range(mask_data.size(0)) 
                        for j in range(mask_data.size(1)) 
                        for k in range(mask_data.size(2)) 
                        for l in range(mask_data.size(3))]
            all_indices.extend(indices)
        
        if not all_masks:
            return
        
        # åˆå¹¶æ‰€æœ‰æ©ç å€¼
        all_masks = torch.cat(all_masks)
        
        # è®¡ç®—å½“å‰å‰ªæç‡ï¼ˆå¦‚æœä½¿ç”¨æ¸è¿›å¼å‰ªæï¼‰
        current_prune_rate = self.current_prune_rate if hasattr(self, 'current_prune_rate') else self.prune_rate
        
        # è®¡ç®—äºŒå€¼åŒ–é˜ˆå€¼
        k = int(all_masks.numel() * current_prune_rate)
        if k == 0:
            return  # æ²¡æœ‰éœ€è¦å‰ªæçš„å…ƒç´ 
        
        # æ‰¾åˆ°é˜ˆå€¼ï¼ˆæœ€å°çš„å‰kä¸ªå€¼ä¸­çš„æœ€å¤§å€¼ï¼‰
        threshold = torch.kthvalue(all_masks, k).values
        
        # è¯†åˆ«ä¿ç•™å’Œå‰ªæçš„æƒé‡
        preserved_mask = all_masks > threshold
        pruned_mask = all_masks <= threshold
        
        preserved_indices = torch.nonzero(preserved_mask, as_tuple=False).squeeze()
        pruned_indices = torch.nonzero(pruned_mask, as_tuple=False).squeeze()
        
        # ç¡®ä¿ç´¢å¼•æ˜¯1Då¼ é‡
        if preserved_indices.dim() == 0:
            preserved_indices = preserved_indices.unsqueeze(0)
        if pruned_indices.dim() == 0:
            pruned_indices = pruned_indices.unsqueeze(0)
        
        # é™åˆ¶æƒé‡äº¤æ¢æ•°é‡
        q_t = min(rate, pruned_indices.size(0), preserved_indices.size(0))
        
        if q_t > 0:
            # é€‰æ‹©è¦äº¤æ¢çš„æƒé‡
            # å‰ªæéƒ¨åˆ†ä¸­æœ€å¤§çš„q_tä¸ªå€¼ï¼ˆæœ€å¯èƒ½æ¢å¤ï¼‰
            _, top_pruned = torch.topk(all_masks[pruned_indices], q_t, largest=True)
            # ä¿ç•™éƒ¨åˆ†ä¸­æœ€å°çš„q_tä¸ªå€¼ï¼ˆæœ€å¯èƒ½å‰ªæï¼‰
            _, bottom_preserved = torch.topk(all_masks[preserved_indices], q_t, largest=False)
            
            # äº¤æ¢æ©ç å€¼
            for idx in top_pruned:
                name = all_names[pruned_indices[idx]]
                indices = all_indices[pruned_indices[idx]]
                
                # è·å–ä¸´æ—¶æ©ç 
                mask_data = temp_masks[name]
                
                # æ›´æ–°æ©ç å€¼ï¼ˆæ¢å¤æƒé‡ï¼‰
                if len(indices) == 1:
                    mask_data[indices[0]] = 1.0
                elif len(indices) == 2:
                    mask_data[indices[0], indices[1]] = 1.0
                elif len(indices) == 4:
                    mask_data[indices[0], indices[1], indices[2], indices[3]] = 1.0
            
            for idx in bottom_preserved:
                name = all_names[preserved_indices[idx]]
                indices = all_indices[preserved_indices[idx]]
                
                # è·å–ä¸´æ—¶æ©ç 
                mask_data = temp_masks[name]
                
                # æ›´æ–°æ©ç å€¼ï¼ˆå‰ªææƒé‡ï¼‰
                if len(indices) == 1:
                    mask_data[indices[0]] = self.eta
                elif len(indices) == 2:
                    mask_data[indices[0], indices[1]] = self.eta
                elif len(indices) == 4:
                    mask_data[indices[0], indices[1], indices[2], indices[3]] = self.eta
        
        # åº”ç”¨äº¤æ¢åçš„æ©ç å€¼åˆ°æ¨¡å‹å‚æ•°
        for name, module in self.prune_layers:
            param_name = f"mask_{name.replace('.', '_')}"
            mask_param = getattr(module, param_name)
            mask_param.data = temp_masks[name]
    
    def validate(self):
        """
        éªŒè¯æ¨¡å‹
        """
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in self.val_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                # å¤„ç†äºŒåˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾
                if self.is_binary:
                    targets = targets.float().view(-1, 1)
                
                outputs = self.forward(inputs)
                loss = self.criterion(outputs, targets)
                
                # è®¡ç®—å‡†ç¡®ç‡
                if self.is_binary:
                    predicted = (torch.sigmoid(outputs) > self.threshold).float()
                    correct += (predicted == targets).sum().item()
                else:
                    _, predicted = outputs.max(1)
                    correct += predicted.eq(targets).sum().item()
                
                total_loss += loss.item()
                total += targets.size(0)
        
        val_loss = total_loss / len(self.val_loader)
        val_acc = 100. * correct / total
        return val_loss, val_acc
    
    def test(self):
        """
        æµ‹è¯•æ¨¡å‹
        """
        self.model.eval()
        y_true = []
        y_pred = []
        
        with torch.no_grad():
            for inputs, targets in self.test_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = self.forward(inputs)
                
                if self.is_binary:
                    # å¤„ç†äºŒåˆ†ç±»é¢„æµ‹
                    preds = (torch.sigmoid(outputs) > self.threshold).int()
                    targets = targets.view(-1, 1).int()
                else:
                    # å¤„ç†å¤šåˆ†ç±»é¢„æµ‹
                    _, preds = outputs.max(1)
                
                y_true.extend(targets.cpu().numpy().flatten())
                y_pred.extend(preds.cpu().numpy().flatten())
        
        test_acc = 100 * accuracy_score(y_true, y_pred)
        return test_acc
    
    def finalize_model(self):
        """
        æœ€ç»ˆåŒ–æ¨¡å‹ï¼šåº”ç”¨æ©ç å¹¶ç§»é™¤æ©ç å‚æ•°
        """
        # åº”ç”¨æœ€ç»ˆæ©ç 
        self.apply_mask()
        
        # ç§»é™¤æ©ç å‚æ•°
        for name, module in self.prune_layers:
            # æ„é€ å®‰å…¨çš„å‚æ•°åç§°
            param_name = f"mask_{name.replace('.', '_')}"
            
            if hasattr(module, param_name):
                # ç§»é™¤æ©ç å‚æ•°
                delattr(module, param_name)
                
                # è§£å†»æƒé‡
                module.weight.requires_grad = True
                
                # å¦‚æœè®¾ç½®äº†å†»ç»“é›¶æƒé‡ï¼Œå†»ç»“æƒé‡å€¼ä¸ºé›¶çš„ä½ç½®
                if self.freeze_zeros:
                    mask = (module.weight.data != 0)
                    module.weight.requires_grad = mask
    
    def train_model(self):
        """
        è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°
        """
        # å‡†å¤‡æ—¥å¿—æ–‡ä»¶
        logger_path = os.path.join(
            self.model_dir,
            f"{self.model_name}_w{self.w}_a{self.a}_e{self.epochs}.log"
        )
        
        with open(logger_path, 'w') as log_file:
            # è®°å½•é…ç½®ä¿¡æ¯
            log_file.write(f"è®­ç»ƒé…ç½®:\n")
            log_file.write(f"æ¨¡å‹: {self.model_name}, æƒé‡ä½å®½: {self.w}, æ¿€æ´»ä½å®½: {self.a}\n")
            log_file.write(f"æ•°æ®é›†: {self.dataset_name}, æ‰¹å¤§å°: {self.batch_size}, è®­ç»ƒè½®æ•°: {self.epochs}\n")
            log_file.write(f"ä¼˜åŒ–å™¨: {self.optimizer_type}, å­¦ä¹ ç‡: {self.lr}, æƒé‡è¡°å‡: {self.weight_decay}\n")
            log_file.write(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.lr_scheduler_type}, è€å¿ƒå€¼: {self.patience}, æœ€å°å­¦ä¹ ç‡: {self.min_lr}\n")
            log_file.write(f"äºŒåˆ†ç±»é˜ˆå€¼: {self.threshold}, æƒé‡è£å‰ª: {self.clip_weights}\n")
            log_file.write(f"å†»ç»“é›¶æƒé‡: {self.freeze_zeros}, å‰ªæç‡: {self.prune_rate}\n")
            log_file.write(f"å‰ªæå±‚: {[name for name, _ in self.prune_layers] if self.prune_layers else 'æ‰€æœ‰å±‚'}\n")
            if self.pretrained_path:
                log_file.write(f"é¢„è®­ç»ƒæƒé‡: {self.pretrained_path}\n")
            log_file.write("\n")
            
            log_file.write(f"{'Epoch':^6} | {'LR':^10} | {'Train Loss':^10} | {'Train Acc':^10} | {'Val Loss':^10} | {'Val Acc':^10} | Status\n")
            log_file.write("-" * 85 + "\n")
            
            best_model = None
            best_val_acc = 0.0
            best_epoch = 0
            
            for epoch in range(1, self.epochs + 1):
                if self.stopped_early:
                    log_file.write(f"â¹ åœ¨ç¬¬ {epoch} è½®æå‰åœæ­¢\n")
                    print(f"â¹ åœ¨ç¬¬ {epoch} è½®æå‰åœæ­¢")
                    break
                
                # è®­ç»ƒ
                train_loss, train_acc = self.train_epoch(epoch)
                
                # éªŒè¯
                val_loss, val_acc = self.validate()
                
                # æ›´æ–°å­¦ä¹ ç‡ (é’ˆå¯¹ReduceLROnPlateau)
                current_lr = self.optimizer.param_groups[0]['lr']
                status = ""
                
                if self.scheduler and isinstance(self.scheduler, ReduceLROnPlateau):
                    self.scheduler.step(val_acc)
                    new_lr = self.optimizer.param_groups[0]['lr']
                    
                    # æ£€æµ‹å­¦ä¹ ç‡æ˜¯å¦å˜åŒ–
                    if new_lr < current_lr:
                        status = "ğŸ”½ LR Reduced"
                        self.epochs_no_improve = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
                        current_lr = new_lr
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_epoch = epoch
                    best_model = copy.deepcopy(self.model.state_dict())
                    self.epochs_no_improve = 0
                    status = "â˜… Best" if not status else status + " â˜… Best"
                else:
                    self.epochs_no_improve += 1
                
                # æ—©åœæ£€æŸ¥
                if self.epochs_no_improve >= self.patience:
                    if current_lr <= self.min_lr:
                        self.stopped_early = True
                        status = "â¹ Early Stop"
                    else:
                        # å¦‚æœè¿˜æœ‰å­¦ä¹ ç©ºé—´ï¼Œåªé‡ç½®æ—©åœè®¡æ•°å™¨
                        self.epochs_no_improve = 0
                
                # è®°å½•æ—¥å¿—
                log_msg = (
                    f"{epoch:^6} | {current_lr:^10.7f} | "
                    f"{train_loss:^10.4f} | {train_acc:^10.2f}% | "
                    f"{val_loss:^10.4f} | {val_acc:^10.2f}% | {status}"
                )
                print(log_msg)
                log_file.write(log_msg + '\n')
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            if best_model:
                self.model.load_state_dict(best_model)
                log_file.write(f"\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (ç¬¬ {best_epoch} è½®)\n")
                print(f"\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (ç¬¬ {best_epoch} è½®)")
            
            # æœ€ç»ˆåŒ–æ¨¡å‹
            self.finalize_model()
            
            # æµ‹è¯•
            test_acc = self.test()
            log_file.write(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%\n")
            print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            model_save_path = os.path.join(
                self.model_dir, 
                f"{self.model_name}_w{self.w}_a{self.a}_e{self.epochs}.pth"
            )
            torch.save(self.model.state_dict(), model_save_path)
            log_file.write(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {model_save_path}\n")
            print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {model_save_path}")
        
        return self.model
if __name__ == "__main__":


    trainer = SRPopupTrainer(
        model_name='2c3f_relu',
        w=4,
        a=4,
        epochs=500,
        random_seed=1998,
        dataset_name='MNIST',
        batch_size=64,
        lr=0.01,
        optimizer='sgd',
        momentum=0.9,
        weight_decay=1e-4,
        lr_scheduler='plateau',
        patience=10,
        threshold=0.5,
        clip_weights=True,
        min_lr=1e-6,
        model=None,  # è‡ªåŠ¨åˆ›å»ºæ¨¡å‹
        freeze_zeros=False,
        prune_rate=0.1,
        prune_layers=None,  # å‰ªææ‰€æœ‰å±‚
        eta=0.99,
        is_binary=False,  # å¤šåˆ†ç±»ä»»åŠ¡
        pretrained_path='pretrained/2c3f_relu_w4_a4_pretrained.pth'  # é¢„è®­ç»ƒæƒé‡è·¯å¾„
    )

    # è®­ç»ƒæ¨¡å‹
    pruned_model = trainer.train_model()