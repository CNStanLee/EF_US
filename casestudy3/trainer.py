import torch.nn.functional as F
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import os
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from models import get_model
from dataset import get_dataloaders
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ç›®å½•é…ç½®
build_dir = "./build"
model_dir = "./model"
data_dir = "./data"

os.makedirs(build_dir, exist_ok=True)
os.makedirs(model_dir, exist_ok=True)
os.makedirs(data_dir, exist_ok=True)

class Trainer:
    def __init__(self, model_name='2c3f', w=4, a=4, epochs=500, random_seed=1998, dataset_name='MNIST',
                 batch_size=64, lr=0.01, optimizer='sgd', momentum=0.9, weight_decay=1e-4,
                 lr_scheduler='plateau', patience=10, threshold=0.5, clip_weights=True, min_lr=1e-6,
                 model=None, freeze_zeros=False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§° (é»˜è®¤ '2c3f')
            w: æƒé‡ä½å®½ (é»˜è®¤ 4)
            a: æ¿€æ´»ä½å®½ (é»˜è®¤ 4)
            epochs: è®­ç»ƒè½®æ•° (é»˜è®¤ 500)
            random_seed: éšæœºç§å­ (é»˜è®¤ 1998)
            dataset_name: æ•°æ®é›†åç§° (é»˜è®¤ 'MNIST')
            batch_size: æ‰¹å¤§å° (é»˜è®¤ 64)
            lr: åˆå§‹å­¦ä¹ ç‡ (é»˜è®¤ 0.01)
            optimizer: ä¼˜åŒ–å™¨ç±»å‹ ('adam' æˆ– 'sgd') (é»˜è®¤ 'adam')
            momentum: SGDåŠ¨é‡ (é»˜è®¤ 0.9)
            weight_decay: æƒé‡è¡°å‡ (é»˜è®¤ 1e-4)
            lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ ('plateau', 'step', 'cosine' æˆ– None) (é»˜è®¤ 'step')
            patience: æ—©åœ/å­¦ä¹ ç‡é™ä½çš„è€å¿ƒå€¼ (é»˜è®¤ 10)
            threshold: äºŒåˆ†ç±»é˜ˆå€¼ (é»˜è®¤ 0.5)
            clip_weights: æ˜¯å¦è£å‰ªæƒé‡ (é»˜è®¤ True)
            min_lr: æœ€å°å­¦ä¹ ç‡ (é»˜è®¤ 1e-6)
            model: é¢„åˆå§‹åŒ–çš„æ¨¡å‹ (é»˜è®¤ None)
            freeze_zeros: æ˜¯å¦å†»ç»“é›¶æƒé‡ (é»˜è®¤ False)
        """
        # åŸºæœ¬å‚æ•°
        self.model_name = model_name
        self.w = w
        self.a = a
        self.epochs = epochs
        self.random_seed = random_seed
        self.dataset_name = dataset_name
        
        # è®­ç»ƒè¶…å‚æ•°
        self.batch_size = batch_size
        self.lr = lr
        self.optimizer_type = optimizer.lower()
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.lr_scheduler_type = lr_scheduler.lower() if lr_scheduler else None
        self.patience = patience
        self.threshold = threshold
        self.clip_weights = clip_weights
        self.min_lr = min_lr
        
        # æ¨¡å‹ç›¸å…³
        self.model = model
        self.freeze_zeros = freeze_zeros
        
        # ç»Ÿä¸€ä»»åŠ¡ç±»å‹åˆ¤æ–­æ ‡å‡†
        self.is_binary = (model_name == 'unsw_fc')
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.best_epoch = 0
        self.epochs_no_improve = 0
        self.stopped_early = False

    def _set_seed(self):
        """è®¾ç½®å…¨å±€éšæœºç§å­"""
        torch.manual_seed(self.random_seed)
        np.random.seed(self.random_seed)
        random.seed(self.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.random_seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False

    def load_pretrained(self, model, weight_path):
        """
        ä»æŒ‡å®šè·¯å¾„åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        å‚æ•°:
            model: è¦åŠ è½½æƒé‡çš„æ¨¡å‹
            weight_path: é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            åŠ è½½äº†é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹
        """
        if not os.path.exists(weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
            
        state_dict = torch.load(weight_path, map_location=device)
        model.load_state_dict(state_dict)
        print(f"loading {weight_path} to weights")
        return model

    def freeze_zero_weights(self, model):
        """å†»ç»“æ¨¡å‹ä¸­å€¼ä¸ºé›¶çš„æƒé‡"""
        for name, param in model.named_parameters():
            if 'weight' in name:
                # åˆ›å»ºæ©ç ï¼šéé›¶ä½ç½®ä¸ºTrueï¼Œé›¶ä½ç½®ä¸ºFalse
                mask = param.data != 0
                # å†»ç»“é›¶æƒé‡ï¼šå°†é›¶æƒé‡ä½ç½®çš„requires_gradè®¾ç½®ä¸ºFalse
                param.requires_grad = mask
                print(f"å†»ç»“ {name} ä¸­çš„é›¶æƒé‡ ({torch.sum(~mask).item()} ä¸ªæƒé‡è¢«å†»ç»“)")

    def train(self, model, loader, criterion, optimizer, device):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in tqdm(loader, desc="Training", leave=False):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            
            if self.is_binary:
                # äºŒåˆ†ç±»å¤„ç†
                labels = labels.float().view(-1, 1)
                loss = criterion(outputs, labels)
                predicted = (torch.sigmoid(outputs) > self.threshold).float()
                correct += (predicted == labels).sum().item()
            else:
                # å¤šåˆ†ç±»å¤„ç†
                loss = criterion(outputs, labels)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
            
            loss.backward()
            
            # åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰åº”ç”¨æ¢¯åº¦æ©ç ï¼ˆå¦‚æœå†»ç»“äº†é›¶æƒé‡ï¼‰
            if self.freeze_zeros:
                for name, param in model.named_parameters():
                    if 'weight' in name and hasattr(param, 'requires_grad_mask'):
                        # ç¡®ä¿æ¢¯åº¦åªåœ¨éé›¶ä½ç½®æ›´æ–°
                        if param.grad is not None:
                            param.grad *= param.requires_grad_mask.float()
            
            optimizer.step()
            
            # æƒé‡è£å‰ª
            if self.clip_weights and hasattr(model, 'clip_weights'):
                model.clip_weights(-1.0, 1.0)
            
            running_loss += loss.item()
            total += labels.size(0)
        
        train_loss = running_loss / len(loader)
        train_acc = 100 * correct / total
        return train_loss, train_acc

    def validate(self, model, loader, criterion, device):
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in tqdm(loader, desc="Validating", leave=False):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                
                if self.is_binary:
                    labels = labels.float().view(-1, 1)
                    loss = criterion(outputs, labels)
                    predicted = (torch.sigmoid(outputs) > self.threshold).float()
                    correct += (predicted == labels).sum().item()
                else:
                    loss = criterion(outputs, labels)
                    _, predicted = torch.max(outputs, 1)
                    correct += (predicted == labels).sum().item()
                
                running_loss += loss.item()
                total += labels.size(0)
        
        val_loss = running_loss / len(loader)
        val_acc = 100 * correct / total
        return val_loss, val_acc

    def test(self, model, test_loader, device):
        """
        æµ‹è¯•æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            model: è¦æµ‹è¯•çš„æ¨¡å‹
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            device: ä½¿ç”¨çš„è®¾å¤‡
            
        è¿”å›:
            æµ‹è¯•å‡†ç¡®ç‡ (ç™¾åˆ†æ¯”)
        """
        model.eval()
        model.to(device)  # ç¡®ä¿æ•´ä¸ªæ¨¡å‹åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
        y_true, y_pred = [], []
        
        with torch.no_grad():
            for inputs, targets in tqdm(test_loader, desc="Testing"):
                # ç¡®ä¿æ•°æ®å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
                inputs = inputs.to(device)
                targets = targets.to(device)
                
                outputs = model(inputs)
                
                if self.is_binary:
                    # ä½¿ç”¨ .detach().cpu() ç¡®ä¿è®¡ç®—åœ¨ CPU ä¸Šè¿›è¡Œ
                    preds = (torch.sigmoid(outputs) > self.threshold).int().detach().cpu().numpy()
                    targets = targets.view(-1, 1).int().detach().cpu().numpy()
                else:
                    _, preds = torch.max(outputs, 1)
                    preds = preds.detach().cpu().numpy()
                    targets = targets.detach().cpu().numpy()
                
                y_true.extend(targets.flatten().tolist())
                y_pred.extend(preds.flatten().tolist())
        
        test_acc = 100 * accuracy_score(y_true, y_pred)
        return test_acc

    def test_model(self, model):
        """
        ç‹¬ç«‹æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
            model: è¦æµ‹è¯•çš„æ¨¡å‹
            
        è¿”å›:
            æµ‹è¯•å‡†ç¡®ç‡ (ç™¾åˆ†æ¯”)
        """
        
        # è·å–æ•°æ®åŠ è½½å™¨
        _, _, test_loader = get_dataloaders(
            dataset_name=self.dataset_name,
            batch_size=self.batch_size
        )
        
        # æ‰§è¡Œæµ‹è¯•
        return self.test(model, test_loader, device)

    def _get_optimizer(self, model):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        # åªä¼˜åŒ–éœ€è¦æ¢¯åº¦çš„å‚æ•°
        params = [p for p in model.parameters() if p.requires_grad]
        
        if not params:
            raise ValueError("æ²¡æœ‰å¯ä¼˜åŒ–çš„å‚æ•°ï¼è¯·æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦è¢«æ­£ç¡®è®¾ç½®ã€‚")
        
        if self.optimizer_type == 'adam':
            return optim.Adam(
                params, 
                lr=self.lr, 
                weight_decay=self.weight_decay
            )
        elif self.optimizer_type == 'sgd':
            return optim.SGD(
                params, 
                lr=self.lr, 
                momentum=self.momentum,
                weight_decay=self.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.optimizer_type}")

    def _get_scheduler(self, optimizer):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.lr_scheduler_type:
            return None
            
        if self.lr_scheduler_type == 'plateau':
            return ReduceLROnPlateau(
                optimizer, 
                mode='max', 
                factor=0.5,
                patience=self.patience // 2,
                verbose=True,
                min_lr=self.min_lr
            )
        elif self.lr_scheduler_type == 'step':
            return StepLR(
                optimizer, 
                step_size=self.epochs // 5, 
                gamma=0.1
            )
        elif self.lr_scheduler_type == 'cosine':
            return CosineAnnealingLR(
                optimizer, 
                T_max=self.epochs,
                eta_min=self.min_lr
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.lr_scheduler_type}")

    def train_model(self):
        """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°"""
        # è®¾ç½®éšæœºç§å­
        self._set_seed()
        
        # åˆå§‹åŒ–æ¨¡å‹
        if self.model is None:
            # å¦‚æœæ²¡æœ‰æä¾›é¢„åˆå§‹åŒ–çš„æ¨¡å‹ï¼Œåˆ™åˆ›å»ºæ–°æ¨¡å‹
            model = get_model(self.model_name, self.w, self.a)
            print(f"åˆ›å»ºæ–°æ¨¡å‹: {self.model_name}")
        else:
            # ä½¿ç”¨æä¾›çš„æ¨¡å‹
            model = self.model
            print(f"ä½¿ç”¨é¢„åˆå§‹åŒ–çš„æ¨¡å‹: {self.model_name}")
        
        model.to(device)
        
        # å†»ç»“é›¶æƒé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.freeze_zeros:
            self.freeze_zero_weights(model)
        
        # è·å–æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = get_dataloaders(
            dataset_name=self.dataset_name,
            batch_size=self.batch_size
        )
        
        # é€‰æ‹©æŸå¤±å‡½æ•°
        criterion = nn.BCEWithLogitsLoss() if self.is_binary else nn.CrossEntropyLoss()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = self._get_optimizer(model)
        scheduler = self._get_scheduler(optimizer)
        
        # å‡†å¤‡æ¨¡å‹ä¿å­˜è·¯å¾„
        model_save_path = os.path.join(
            model_dir, 
            f"{self.model_name}_w{self.w}_a{self.a}_e{self.epochs}.pth"
        )
        best_model_save_path = os.path.join(
            model_dir, 
            f"best_{self.model_name}_w{self.w}_a{self.a}_e{self.epochs}.pth"
        )
        logger_path = os.path.join(
            model_dir,
            f"{self.model_name}_w{self.w}_a{self.a}_e{self.epochs}.log"
        )
        
        # è®­ç»ƒå¾ªç¯
        self.best_val_acc = 0.0
        self.best_epoch = 0
        self.epochs_no_improve = 0
        self.stopped_early = False
        
        with open(logger_path, 'w') as log_file:
            # è®°å½•é…ç½®ä¿¡æ¯
            log_file.write(f"è®­ç»ƒé…ç½®:\n")
            log_file.write(f"æ¨¡å‹: {self.model_name}, æƒé‡ä½å®½: {self.w}, æ¿€æ´»ä½å®½: {self.a}\n")
            log_file.write(f"æ•°æ®é›†: {self.dataset_name}, æ‰¹å¤§å°: {self.batch_size}, è®­ç»ƒè½®æ•°: {self.epochs}\n")
            log_file.write(f"ä¼˜åŒ–å™¨: {self.optimizer_type}, å­¦ä¹ ç‡: {self.lr}, æƒé‡è¡°å‡: {self.weight_decay}\n")
            log_file.write(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.lr_scheduler_type}, è€å¿ƒå€¼: {self.patience}, æœ€å°å­¦ä¹ ç‡: {self.min_lr}\n")
            log_file.write(f"äºŒåˆ†ç±»é˜ˆå€¼: {self.threshold}, æƒé‡è£å‰ª: {self.clip_weights}\n")
            log_file.write(f"å†»ç»“é›¶æƒé‡: {self.freeze_zeros}\n\n")
            
            log_file.write(f"{'Epoch':^6} | {'LR':^10} | {'Train Loss':^10} | {'Train Acc':^10} | {'Val Loss':^10} | {'Val Acc':^10} | Status\n")
            log_file.write("-" * 85 + "\n")
            
            for epoch in range(self.epochs):
                if self.stopped_early:
                    log_file.write(f"â¹ åœ¨ç¬¬ {epoch} è½®æå‰åœæ­¢\n")
                    print(f"â¹ åœ¨ç¬¬ {epoch} è½®æå‰åœæ­¢")
                    break
                
                # è®­ç»ƒå’ŒéªŒè¯
                train_loss, train_acc = self.train(
                    model, train_loader, criterion, optimizer, device
                )
                val_loss, val_acc = self.validate(
                    model, val_loader, criterion, device
                )
                
                # æ›´æ–°å­¦ä¹ ç‡
                current_lr = optimizer.param_groups[0]['lr']
                status = ""
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹
                if val_acc > self.best_val_acc:
                    self.best_val_acc = val_acc
                    self.best_epoch = epoch
                    torch.save(model.state_dict(), best_model_save_path)
                    self.epochs_no_improve = 0
                    status = "â˜… Best"
                else:
                    self.epochs_no_improve += 1
                
                # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä»…é’ˆå¯¹Plateauï¼‰
                if scheduler is not None and isinstance(scheduler, ReduceLROnPlateau):
                    scheduler.step(val_acc)  # ä¼ å…¥ç›‘æ§æŒ‡æ ‡
                    new_lr = optimizer.param_groups[0]['lr']
                    
                    # æ£€æµ‹å­¦ä¹ ç‡æ˜¯å¦å˜åŒ–
                    if new_lr < current_lr:
                        status = "ğŸ”½ LR Reduced"
                        self.epochs_no_improve = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
                        current_lr = new_lr
                
                # å…¶ä»–ç±»å‹å­¦ä¹ ç‡è°ƒåº¦
                elif scheduler is not None:
                    scheduler.step()
                    current_lr = optimizer.param_groups[0]['lr']
                
                # æ—©åœæ£€æŸ¥ï¼ˆç‹¬ç«‹äºå­¦ä¹ ç‡è°ƒåº¦ï¼‰
                if self.epochs_no_improve >= self.patience:
                    if current_lr <= self.min_lr:
                        self.stopped_early = True
                        status = "â¹ Early Stop"
                    else:
                        # å¦‚æœè¿˜æœ‰å­¦ä¹ ç©ºé—´ï¼Œåªé‡ç½®æ—©åœè®¡æ•°å™¨
                        self.epochs_no_improve = 0
                
                # è®°å½•æ—¥å¿—
                log_msg = (
                    f"{epoch+1:^6} | {current_lr:^10.7f} | "
                    f"{train_loss:^10.4f} | {train_acc:^10.2f}% | "
                    f"{val_loss:^10.4f} | {val_acc:^10.2f}% | {status}"
                )
                print(log_msg)
                log_file.write(log_msg + '\n')
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
        model.load_state_dict(torch.load(best_model_save_path))
        test_acc = self.test(model, test_loader, device)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(model.state_dict(), model_save_path)
        
        # æœ€ç»ˆæ—¥å¿—
        final_msg = (
            f"\nè®­ç»ƒå®Œæˆ: æœ€ä½³éªŒè¯å‡†ç¡®ç‡ {self.best_val_acc:.2f}% (ç¬¬ {self.best_epoch+1} è½®)\n"
            f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%\n"
            f"æœ€ä½³æ¨¡å‹ä¿å­˜è‡³: {best_model_save_path}\n"
            f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {model_save_path}"
        )
        
        print(final_msg)
        with open(logger_path, 'a') as log_file:
            log_file.write(final_msg + '\n')
        
        return model
